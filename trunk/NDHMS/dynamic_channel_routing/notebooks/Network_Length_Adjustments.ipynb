{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "try:\n",
    "    import google.colab\n",
    "    ENV_IS_CL = True\n",
    "    !git clone --single-branch --branch network https://github.com/jameshalgren/wrf_hydro_nwm_public.git\n",
    "    sys.path.append('/content/wrf_hydro_nwm_public/trunk/NDHMS/dynamic_channel_routing/src/python_framework')\n",
    "    !pip install geopandas\n",
    "    !pip install netcdf4\n",
    "    #default recursion limit (~1000) is slightly too small for the deepest branches of the network\n",
    "    sys.setrecursionlimit(6000) \n",
    "    #TODO: convert recursive functions to stack-based functions\n",
    "except:\n",
    "    ENV_IS_CL = False\n",
    "    sys.path.append(r'../src/python_framework')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkbuilder as networkbuilder\n",
    "import recursive_print\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"NHD Network traversal\n",
    "\n",
    "A demonstration version of this code is stored in this Colaboratory notebook:\n",
    "https://colab.research.google.com/github/jameshalgren/wrf_hydro_nwm_public/blob/network/trunk/NDHMS/dynamic_channel_routing/notebooks/NHD_Network_Density_Analysis.ipynb#scrollTo=h_BEdl4LID34\n",
    "\n",
    "\"\"\"\n",
    "def do_network(\n",
    "        geofile_path = None\n",
    "        , title_string = None\n",
    "        , layer_string = None\n",
    "        , driver_string = None\n",
    "        , key_col = None\n",
    "        , downstream_col = None\n",
    "        , length_col = None\n",
    "        , terminal_code = None\n",
    "        , verbose = False\n",
    "        , debuglevel = 0\n",
    "        ):\n",
    "\n",
    "    # NOTE: these methods can lose the \"connections\" and \"rows\" arguments when\n",
    "    # implemented as class methods where those arrays are members of the class.\n",
    "    if verbose: print(title_string)\n",
    "    if driver_string == 'NetCDF':\n",
    "        geofile = xr.open_dataset(geofile_path)\n",
    "        geofile_rows = (geofile.to_dataframe()).values\n",
    "        # The xarray method for NetCDFs was implemented after the geopandas method for \n",
    "        # GIS source files. It's possible (probable?) that we are doing something \n",
    "        # inefficient by converting away from the Pandas dataframe.\n",
    "        # TODO: Check the optimal use of the Pandas dataframe\n",
    "        if debuglevel <= -1: print(f'reading -- dataset: {geofile_path}; layer: {layer_string}; driver: {driver_string}')\n",
    "    else:\n",
    "        if debuglevel <= -1: print(f'reading -- dataset: {geofile_path}; layer: {layer_string}; fiona driver: {driver_string}')\n",
    "        geofile = gpd.read_file(geofile_path, driver=driver_string, layer=layer_string)\n",
    "        geofile_rows = geofile.to_numpy()\n",
    "    if debuglevel <= -2: geofile.plot() #TODO: WILL THIS WORK WITH NetCDF???\n",
    "    if debuglevel <= -1: print(geofile.head()) #TODO: WILL THIS WORK WITH NetCDF???\n",
    "    # Kick off recursive call for all connections and keys\n",
    "    (connections) = networkbuilder.get_down_connections(\n",
    "                    rows = geofile_rows\n",
    "                    , key_col = key_col\n",
    "                    , downstream_col = downstream_col\n",
    "                    , length_col = length_col\n",
    "                    , verbose = verbose\n",
    "                    , debuglevel = debuglevel)\n",
    "    \n",
    "    (all_keys, ref_keys, headwater_keys\n",
    "        , terminal_keys\n",
    "        , terminal_ref_keys\n",
    "        , circular_keys) = networkbuilder.determine_keys(\n",
    "                    connections = connections\n",
    "#                     , rows = geofile_rows\n",
    "                    , key_col = key_col\n",
    "                    , downstream_col = downstream_col\n",
    "                    , terminal_code = terminal_code\n",
    "                    , verbose = verbose\n",
    "                    , debuglevel = debuglevel)\n",
    "    \n",
    "    (junction_keys) = networkbuilder.get_up_connections(\n",
    "                    connections = connections\n",
    "                    , terminal_code = terminal_code\n",
    "                    , headwater_keys = headwater_keys\n",
    "                    , terminal_keys = terminal_keys\n",
    "                    , verbose = verbose\n",
    "                    , debuglevel = debuglevel)\n",
    "    return connections, all_keys, ref_keys, headwater_keys \\\n",
    "        , terminal_keys, terminal_ref_keys \\\n",
    "        , circular_keys, junction_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_print():    \n",
    "    recursive_print.print_basic_network_info(\n",
    "                    connections = connections_NHD\n",
    "                    , headwater_keys = headwater_keys_NHD\n",
    "                    , junction_keys = junction_keys_NHD\n",
    "                    , terminal_keys = terminal_keys_NHD\n",
    "                    , terminal_code = terminal_code_NHD\n",
    "                    , verbose = True\n",
    "                    )\n",
    "    \n",
    "    if 1 == 0: #THE RECURSIVE PRINT IS NOT A GOOD IDEA WITH LARGE NETWORKS!!!\n",
    "        recursive_print.print_connections(\n",
    "                    headwater_keys = headwater_keys_NHD\n",
    "                    , down_connections = connections_NHD\n",
    "                    , up_connections = connections_NHD\n",
    "                    , terminal_code = terminal_code_NHD\n",
    "                    , terminal_keys = terminal_keys_NHD\n",
    "                    , terminal_ref_keys = terminal_ref_keys_NHD\n",
    "                    , debuglevel = -2\n",
    "                    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Real Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONUS Order 5 and Greater \n",
      "reading -- dataset: /home/jacob.hreha/nwm/trunk/NDHMS/dynamic_channel_routing/test/input/geo/Channels/NHD_BrazosLowerColorado_Channels.shp; layer: 0; fiona driver: ESRI Shapefile\n",
      "   OBJECTID_1  OBJECTID  featureID  linkDim     link  order_  Length       to  \\\n",
      "0        1499     25442    3764288   460086  3764288       6  2150.0  3764296   \n",
      "1        1500     25443    3764296   460088  3764296       6  1277.0  3765756   \n",
      "2        1501     25444    3766380   460092  3766380       6   431.0  3766382   \n",
      "3        1502     25445    3765756   460090  3765756       6  1274.0  3766380   \n",
      "4        1503     25447    3765796   460180  3765796       6   219.0  3765798   \n",
      "\n",
      "     MusK  MusX  ...  gages  NHDWaterbo      lat      lon   alt Kchan  \\\n",
      "0  3600.0   0.2  ...   None       -9999  29.0176 -96.0119  9.59     0   \n",
      "1  3600.0   0.2  ...   None       -9999  29.0059 -96.0029  9.59     0   \n",
      "2  3600.0   0.2  ...   None       -9999  28.9876 -95.9992  9.59     0   \n",
      "3  3600.0   0.2  ...   None       -9999  28.9946 -96.0019  9.59     0   \n",
      "4  3600.0   0.2  ...   None       -9999  28.7625 -96.0020  1.83     0   \n",
      "\n",
      "   ascendingI  Shape_Leng  Shape_Le_1  \\\n",
      "0     2034014    0.021074    0.021074   \n",
      "1     2034015    0.011898    0.011898   \n",
      "2     2071131    0.003975    0.003975   \n",
      "3     2034016    0.011638    0.011638   \n",
      "4     2071148    0.002016    0.002016   \n",
      "\n",
      "                                            geometry  \n",
      "0  LINESTRING (-96.02196 29.02037, -96.02186 29.0...  \n",
      "1  LINESTRING (-96.00667 29.01034, -96.00620 29.0...  \n",
      "2  LINESTRING (-96.00024 28.98924, -95.99997 28.9...  \n",
      "3  LINESTRING (-96.00117 29.00026, -96.00103 28.9...  \n",
      "4  LINESTRING (-96.00230 28.76346, -96.00197 28.7...  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "down connections ...\n",
      "found 2014 segments\n",
      "down_connections complete\n",
      "ref_keys ...\n",
      "found 2000 ref_keys\n",
      "ref_keys complete\n",
      "headwater_keys ...\n",
      "found 15 headwater segments\n",
      "headwater_keys complete\n",
      "terminal_keys ...\n",
      "found 2 terminal segments\n",
      "of those, 0 had non-standard terminal keys\n",
      "terminal_keys complete\n",
      "circular_keys ...\n",
      "identified at least 0 segments with circular references testing to the fourth level\n",
      "circular_keys complete\n",
      "identifying upstream connections and junctions ...\n",
      "visited 2014 segments\n",
      "found 13 junctions in 13 junction nodes\n",
      "up_connections complete\n",
      "\n",
      "CONUS Order 5 and Greater \n",
      "reading -- dataset: /home/jacob.hreha/nwm/trunk/NDHMS/dynamic_channel_routing/test/input/geo/Channels/NHD_Conus_Channels.shp; layer: 0; fiona driver: ESRI Shapefile\n",
      "   OBJECTID  featureID  linkDim    link  order_  Length          to    MusK  \\\n",
      "0      2283     328271   388099  328271       8  6685.0      328275  3600.0   \n",
      "1      2284     328275   388100  328275       8  4464.0  1131002478  3600.0   \n",
      "2      2287     328285   388172  328285       8  2291.0  1131002488  3600.0   \n",
      "3      2290     625322   392961  625322       8   441.0      625372  3600.0   \n",
      "4      2291     625326   392956  625326       8   966.0      625370  3600.0   \n",
      "\n",
      "   MusX   So  ...   Qi  gages  NHDWaterbo      lat      lon    alt  Kchan  \\\n",
      "0   0.2  0.0  ...  0.0   None       -9999  27.1069 -99.4416  92.41      0   \n",
      "1   0.2  0.0  ...  0.0   None       -9999  27.0684 -99.4481  91.68      0   \n",
      "2   0.2  0.0  ...  0.0   None   120051895  26.9912 -99.3936  91.68      0   \n",
      "3   0.2  0.0  ...  0.0   None       -9999  26.1160 -98.2670  24.86      0   \n",
      "4   0.2  0.0  ...  0.0   None       -9999  26.1094 -98.3036  25.52      0   \n",
      "\n",
      "   ascendingI  Shape_Leng                                           geometry  \n",
      "0     1337104    0.062148  LINESTRING (-99.43063 27.13471, -99.43124 27.1...  \n",
      "1     1191151    0.041935  LINESTRING (-99.43195 27.08172, -99.43532 27.0...  \n",
      "2     1317845    0.021690  LINESTRING (-99.39823 27.00032, -99.39863 26.9...  \n",
      "3     1319219    0.004276  LINESTRING (-98.26538 26.11741, -98.26544 26.1...  \n",
      "4     1316348    0.009040  LINESTRING (-98.30100 26.10593, -98.30102 26.1...  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "down connections ...\n",
      "found 171887 segments\n",
      "down_connections complete\n",
      "ref_keys ...\n",
      "found 169383 ref_keys\n",
      "ref_keys complete\n",
      "headwater_keys ...\n",
      "found 2505 headwater segments\n",
      "headwater_keys complete\n",
      "terminal_keys ...\n",
      "found 355 terminal segments\n",
      "of those, 0 had non-standard terminal keys\n",
      "terminal_keys complete\n",
      "circular_keys ...\n",
      "identified at least 0 segments with circular references testing to the fourth level\n",
      "circular_keys complete\n",
      "identifying upstream connections and junctions ...\n",
      "visited 171887 segments\n",
      "found 2150 junctions in 2148 junction nodes\n",
      "up_connections complete\n",
      "\n",
      "CONUS Full Resolution NWM v2.0\n",
      "reading -- dataset: /home/jacob.hreha/nwm/trunk/NDHMS/dynamic_channel_routing/test/input/geo/Channels/RouteLink_NWMv2.0_20190517_cheyenne_pull.nc; layer: 0; driver: NetCDF\n",
      "<xarray.Dataset>\n",
      "Dimensions:            (feature_id: 5)\n",
      "Coordinates:\n",
      "    lon                (feature_id) float32 -96.5402 -96.53065 ... -96.63716\n",
      "    lat                (feature_id) float32 46.228783 46.213486 ... 46.195522\n",
      "Dimensions without coordinates: feature_id\n",
      "Data variables:\n",
      "    link               (feature_id) int32 6635572 6635590 ... 6635622 6635626\n",
      "    from               (feature_id) int32 0 0 0 0 0\n",
      "    to                 (feature_id) int32 6635570 6635600 ... 6635620 6635624\n",
      "    alt                (feature_id) float32 294.75 295.89 296.15 292.49 294.39\n",
      "    order              (feature_id) int32 1 1 1 1 1\n",
      "    Qi                 (feature_id) float32 0.0 0.0 0.0 0.0 0.0\n",
      "    MusK               (feature_id) float32 3600.0 3600.0 3600.0 3600.0 3600.0\n",
      "    MusX               (feature_id) float32 0.2 0.2 0.2 0.2 0.2\n",
      "    Length             (feature_id) float32 1070.0 1117.0 2303.0 1119.0 3171.0\n",
      "    n                  (feature_id) float32 0.06 0.06 0.06 0.06 0.06\n",
      "    So                 (feature_id) float32 0.001 1e-05 1e-05 1e-05 0.001\n",
      "    ChSlp              (feature_id) float32 0.6912526 0.7562847 ... 0.47519878\n",
      "    BtmWdth            (feature_id) float32 1.9663649 1.6038141 ... 4.598223\n",
      "    NHDWaterbodyComID  (feature_id) int32 -9999 -9999 -9999 -9999 -9999\n",
      "    time               datetime64[ns] 2000-01-01\n",
      "    gages              (feature_id) |S15 b'               ' ... b'               '\n",
      "    Kchan              (feature_id) int16 0 0 0 0 0\n",
      "    ascendingIndex     (feature_id) int32 457632 2155781 2174794 2193033 2197068\n",
      "    nCC                (feature_id) float32 0.12 0.12 0.12 0.12 0.12\n",
      "    TopWdthCC          (feature_id) float32 9.831824 8.019071 ... 22.991117\n",
      "    TopWdth            (feature_id) float32 3.2772746 2.6730235 ... 7.663706\n",
      "Attributes:\n",
      "    Convention:        CF-1.6\n",
      "    featureType:       timeSeries\n",
      "    history:           Created Wed Nov 07 23:45:44 2018\n",
      "    processing_notes:  This file was produced Wed Nov 07 17:14:48 2018 by Kev...\n",
      "down connections ...\n",
      "found 2729077 segments\n",
      "down_connections complete\n",
      "ref_keys ...\n",
      "found 1685497 ref_keys\n",
      "ref_keys complete\n",
      "headwater_keys ...\n",
      "found 1043581 headwater segments\n",
      "headwater_keys complete\n",
      "terminal_keys ...\n",
      "found 14351 terminal segments\n",
      "of those, 0 had non-standard terminal keys\n",
      "terminal_keys complete\n",
      "circular_keys ...\n",
      "identified at least 0 segments with circular references testing to the fourth level\n",
      "circular_keys complete\n",
      "identifying upstream connections and junctions ...\n",
      "visited 2729077 segments\n",
      "found 1029230 junctions in 1023059 junction nodes\n",
      "up_connections complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if ENV_IS_CL: root = '/content/wrf_hydro_nwm_public/trunk/NDHMS/dynamic_channel_routing/'\n",
    "elif not ENV_IS_CL: root = os.path.dirname(os.path.abspath(''))\n",
    "test_folder = os.path.join(root, r'test')\n",
    "geo_input_folder = os.path.join(test_folder, r'input', r'geo', r'Channels')\n",
    "\n",
    "\"\"\"##NHD Subset (Brazos/Lower Colorado)\"\"\"\n",
    "Brazos_LowerColorado_ge5 = True\n",
    "\"\"\"##NHD CONUS order 5 and greater\"\"\"\n",
    "CONUS_ge5 = True\n",
    "\"\"\"These are large -- be careful\"\"\"\n",
    "CONUS_FULL_RES = True\n",
    "CONUS_Named_Streams = False #create a subset of the full resolution by reading the GNIS field\n",
    "CONUS_Named_combined = False #process the Named streams through the Full-Res paths to join the many hanging reaches\n",
    "\n",
    "debuglevel = -1\n",
    "verbose = True\n",
    "\n",
    "# The CONUS_ge5 and Brazos_LowerColorado_ge5 datasets are included\n",
    "# in the github test folder and are extracts from the NHD version 1.2 datasets\n",
    "# from https://www.nohrsc.noaa.gov/pub/staff/keicher/NWM_live/web/data_tools/\n",
    "#  \n",
    "# The CONUS_FULL_RES file was generated from the RouteLink file in the parameter\n",
    "# archive and converted to a compressed NetCDF via the following command:\n",
    "# nccopy -d1 -s RouteLink_NWMv2.0_20190517_cheyenne_pull.nc RouteLink_NWMv2.0_20190517_cheyenne_pull.nc4s\n",
    "# TODO: Explain CONUS_Named_Streams\n",
    "# CONUS_Named_Streams was generated by intersecting the FULL_RES file ...\n",
    "# of the data in the nohrsc-hosted archive but are too large to efficiently \n",
    "# package inside of the repository. \n",
    "\n",
    "if Brazos_LowerColorado_ge5:\n",
    "    nhd_conus_file_path = os.path.join(geo_input_folder\n",
    "            , r'NHD_BrazosLowerColorado_Channels.shp')\n",
    "    key_col_NHD = 2\n",
    "    downstream_col_NHD = 7\n",
    "    length_col_NHD = 6\n",
    "    terminal_code_NHD = 0\n",
    "    title_string = 'Brazos + Lower Colorado\\nNHD stream orders 5 and greater\\n'\n",
    "    title_string = 'CONUS Order 5 and Greater '\n",
    "    driver_string = 'ESRI Shapefile'\n",
    "    layer_string = 0\n",
    "\n",
    "    Brazos_LowerColorado_ge5_values = do_network (nhd_conus_file_path\n",
    "                , title_string = title_string\n",
    "                , layer_string = layer_string\n",
    "                , driver_string = driver_string\n",
    "                , key_col = key_col_NHD\n",
    "                , downstream_col = downstream_col_NHD\n",
    "                , length_col = length_col_NHD\n",
    "                , terminal_code = terminal_code_NHD\n",
    "                , verbose = verbose\n",
    "                , debuglevel = debuglevel)\n",
    "\n",
    "if CONUS_ge5:\n",
    "    nhd_conus_file_path = os.path.join(geo_input_folder\n",
    "            , r'NHD_Conus_Channels.shp')\n",
    "    key_col_NHD = 1\n",
    "    downstream_col_NHD = 6\n",
    "    length_col_NHD = 5\n",
    "    terminal_code_NHD = 0\n",
    "    title_string = 'CONUS Order 5 and Greater '\n",
    "    driver_string = 'ESRI Shapefile'\n",
    "    layer_string = 0\n",
    "\n",
    "    CONUS_ge5_values = do_network (nhd_conus_file_path\n",
    "                , title_string = title_string\n",
    "                , layer_string = layer_string\n",
    "                , driver_string = driver_string\n",
    "                , key_col = key_col_NHD\n",
    "                , downstream_col = downstream_col_NHD\n",
    "                , length_col = length_col_NHD\n",
    "                , terminal_code = terminal_code_NHD\n",
    "                , verbose = verbose\n",
    "                , debuglevel = debuglevel)\n",
    "\n",
    "if CONUS_FULL_RES:\n",
    "    # nhd_conus_file_path = '../../../../../../GISTemp/nwm_v12.gdb'\n",
    "    nhd_conus_file_path = os.path.join(geo_input_folder\n",
    "            , r'RouteLink_NWMv2.0_20190517_cheyenne_pull.nc')\n",
    "    key_col_NHD = 0\n",
    "    downstream_col_NHD = 2\n",
    "    length_col_NHD = 10\n",
    "    terminal_code_NHD = 0\n",
    "    title_string = 'CONUS Full Resolution NWM v2.0'\n",
    "    # driver_string = 'FileGDB'\n",
    "    driver_string = 'NetCDF'\n",
    "    # layer_string = 'channels_nwm_v12_routeLink'\n",
    "    layer_string = 0\n",
    "\n",
    "    CONUS_FULL_RES_values = do_network (nhd_conus_file_path\n",
    "                , title_string = title_string\n",
    "                , layer_string = layer_string\n",
    "                , driver_string = driver_string\n",
    "                , key_col = key_col_NHD\n",
    "                , downstream_col = downstream_col_NHD\n",
    "                , length_col = length_col_NHD\n",
    "                , terminal_code = terminal_code_NHD\n",
    "                , verbose = verbose\n",
    "                , debuglevel = debuglevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Executing Test\n",
      "down connections ...\n",
      "found 16 segments\n",
      "{0: {'downstream': None, 'length': 456}, 1: {'downstream': 4, 'length': 678}, 2: {'downstream': 0, 'length': 394}, 3: {'downstream': 2, 'length': 815}, 4: {'downstream': 0, 'length': 798}, 5: {'downstream': 4, 'length': 679}, 6: {'downstream': 0, 'length': 394}, 7: {'downstream': 2, 'length': 815}, 8: {'downstream': None, 'length': 841}, 9: {'downstream': 12, 'length': 524}, 10: {'downstream': 9, 'length': 458}, 11: {'downstream': 8, 'length': 548}, 12: {'downstream': 8, 'length': 543}, 13: {'downstream': 14, 'length': 458}, 14: {'downstream': 10, 'length': 548}, 15: {'downstream': 14, 'length': 543}}\n",
      "down_connections complete\n",
      "ref_keys ...\n",
      "found 9 ref_keys\n",
      "{0, 2, 4, None, 8, 9, 10, 12, 14}\n",
      "ref_keys complete\n",
      "{0, 2, 4, None, 8, 9, 10, 12, 14}\n",
      "headwater_keys ...\n",
      "found 8 headwater segments\n",
      "{1, 3, 5, 6, 7, 11, 13, 15}\n",
      "headwater_keys complete\n",
      "terminal_keys ...\n",
      "Non-standard terminal key None found in segment 0\n",
      "Non-standard terminal key None found in segment 8\n",
      "found 2 terminal segments\n",
      "of those, 1 had non-standard terminal keys\n",
      "{0, 8}\n",
      "terminal_keys complete\n",
      "circular_keys ...\n",
      "identified at least 0 segments with circular references testing to the fourth level\n",
      "set()\n",
      "circular_keys complete\n",
      "identifying upstream connections and junctions ...\n",
      "Junction found above/into Segment 0 with upstream Segments {2, 4}\n",
      "Junction found above/into Segment 4 with upstream Segments {1, 5}\n",
      "revisited Junction above/into Segment 0 now with upstream Segments {2, 4, 6}\n",
      "Junction found above/into Segment 2 with upstream Segments {3, 7}\n",
      "Junction found above/into Segment 8 with upstream Segments {11, 12}\n",
      "Junction found above/into Segment 14 with upstream Segments {13, 15}\n",
      "visited 16 segments\n",
      "found 6 junctions in 5 junction nodes\n",
      "{0, 2, 4, 8, 14}\n",
      "{0: {'downstream': None, 'length': 456, 'upstreams': {2, 4, 6}}, 1: {'downstream': 4, 'length': 678, 'upstreams': {-999}}, 2: {'downstream': 0, 'length': 394, 'upstreams': {3, 7}}, 3: {'downstream': 2, 'length': 815, 'upstreams': {-999}}, 4: {'downstream': 0, 'length': 798, 'upstreams': {1, 5}}, 5: {'downstream': 4, 'length': 679, 'upstreams': {-999}}, 6: {'downstream': 0, 'length': 394, 'upstreams': {-999}}, 7: {'downstream': 2, 'length': 815, 'upstreams': {-999}}, 8: {'downstream': None, 'length': 841, 'upstreams': {11, 12}}, 9: {'downstream': 12, 'length': 524, 'upstreams': {10}}, 10: {'downstream': 9, 'length': 458, 'upstreams': {14}}, 11: {'downstream': 8, 'length': 548, 'upstreams': {-999}}, 12: {'downstream': 8, 'length': 543, 'upstreams': {9}}, 13: {'downstream': 14, 'length': 458, 'upstreams': {-999}}, 14: {'downstream': 10, 'length': 548, 'upstreams': {13, 15}}, 15: {'downstream': 14, 'length': 543, 'upstreams': {-999}}}\n",
      "up_connections complete\n",
      "\n",
      "########################\n",
      "Downstream Connections\n",
      "########################\n",
      "1 with length 678\n",
      "4 with length 798\n",
      "0 with length 456\n",
      "########################\n",
      "3 with length 815\n",
      "2 with length 394\n",
      "0 with length 456\n",
      "########################\n",
      "5 with length 679\n",
      "4 with length 798\n",
      "0 with length 456\n",
      "########################\n",
      "6 with length 394\n",
      "0 with length 456\n",
      "########################\n",
      "7 with length 815\n",
      "2 with length 394\n",
      "0 with length 456\n",
      "########################\n",
      "11 with length 548\n",
      "8 with length 841\n",
      "########################\n",
      "13 with length 458\n",
      "14 with length 548\n",
      "10 with length 458\n",
      "9 with length 524\n",
      "12 with length 543\n",
      "8 with length 841\n",
      "########################\n",
      "15 with length 543\n",
      "14 with length 548\n",
      "10 with length 458\n",
      "9 with length 524\n",
      "12 with length 543\n",
      "8 with length 841\n",
      "########################\n",
      "########################\n",
      "Upstream Connections\n",
      "########################\n",
      "\\0 with length 456\\\n",
      "..\\2 with length 394\\\n",
      "....\\3 with length 815\\\n",
      "....\\7 with length 815\\\n",
      "..\\4 with length 798\\\n",
      "....\\1 with length 678\\\n",
      "....\\5 with length 679\\\n",
      "..\\6 with length 394\\\n",
      "########################\n",
      "\\8 with length 841\\\n",
      "..\\11 with length 548\\\n",
      "..\\12 with length 543\\\n",
      "....\\9 with length 524\\\n",
      "......\\10 with length 458\\\n",
      "........\\14 with length 548\\\n",
      "..........\\13 with length 458\\\n",
      "..........\\15 with length 543\\\n",
      "########################\n",
      "junction# 2 at key 0 has these upstream segments {2, 4, 6}\n",
      "junction# 3 at key 2 has these upstream segments {3, 7}\n",
      "junction# 4 at key 4 has these upstream segments {1, 5}\n",
      "junction# 5 at key 8 has these upstream segments {11, 12}\n",
      "junction# 6 at key 14 has these upstream segments {13, 15}\n",
      "Total Segments 16\n",
      "Head Segments 8\n",
      "found 6 junctions\n",
      "...in 4 junction nodes\n",
      "Total Reaches ( = head_segments + junction_nodes ) 12\n",
      "Total Independent Networks (estimated from number of terminal keys) 2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "if 1 == 1:\n",
    "    \"\"\"##TEST\"\"\"\n",
    "    print(\"\")\n",
    "    print ('Executing Test')\n",
    "    # Test data\n",
    "    test_rows = [\n",
    "    [0,456,None,0],\n",
    "    [1,678,4,0],\n",
    "    [2,394,0,0],\n",
    "    [3,815,2,0],\n",
    "    [4,798,0,0],\n",
    "    [5,679,4,0],\n",
    "    [6,394,0,0],\n",
    "    [7,815,2,0],\n",
    "    [8,841,None,0],\n",
    "    [9,524,12,0],\n",
    "    [10,458,9,0],\n",
    "    [11,548,8,0],\n",
    "    [12,543,8,0],\n",
    "    [13,458,14,0],\n",
    "    [14,548,10,0],\n",
    "    [15,543,14,0],\n",
    "]\n",
    "\n",
    "    test_key_col = 0\n",
    "    test_downstream_col = 2\n",
    "    test_length_col = 1\n",
    "    test_terminal_code = -999\n",
    "    debuglevel = -2\n",
    "    verbose = True\n",
    "\n",
    "    (test_connections) = networkbuilder.get_down_connections(\n",
    "                rows = test_rows\n",
    "                , key_col = test_key_col\n",
    "                , downstream_col = test_downstream_col\n",
    "                , length_col = test_length_col\n",
    "                , verbose = True\n",
    "                , debuglevel = debuglevel\n",
    "                )\n",
    "\n",
    "    (test_all_keys, test_ref_keys, test_headwater_keys\n",
    "     , test_terminal_keys\n",
    "     , test_terminal_ref_keys\n",
    "     , test_circular_keys) = networkbuilder.determine_keys(\n",
    "                connections = test_connections\n",
    "                \n",
    "                , key_col = test_key_col\n",
    "                , downstream_col = test_downstream_col\n",
    "                , terminal_code = test_terminal_code\n",
    "                , verbose = True\n",
    "                , debuglevel = debuglevel\n",
    "                )\n",
    "\n",
    "    test_junction_keys = networkbuilder.get_up_connections(\n",
    "                connections = test_connections\n",
    "                , terminal_code = test_terminal_code\n",
    "                , headwater_keys = test_headwater_keys\n",
    "                , terminal_keys = test_terminal_keys\n",
    "                , verbose = True\n",
    "                , debuglevel = debuglevel\n",
    "                )\n",
    "\n",
    "    recursive_print.print_connections(\n",
    "                headwater_keys = test_headwater_keys\n",
    "                , down_connections = test_connections\n",
    "                , up_connections = test_connections\n",
    "                , terminal_code = test_terminal_code\n",
    "                , terminal_keys = test_terminal_keys\n",
    "                , terminal_ref_keys = test_terminal_ref_keys\n",
    "                , debuglevel = debuglevel\n",
    "                )\n",
    "\n",
    "    recursive_print.print_basic_network_info(\n",
    "                connections = test_connections\n",
    "                , headwater_keys = test_headwater_keys\n",
    "                , junction_keys = test_junction_keys\n",
    "                , terminal_keys = test_terminal_keys\n",
    "                , terminal_code = test_terminal_code\n",
    "                , verbose = True\n",
    "                , debuglevel = debuglevel\n",
    "                )\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.00010943412780761719 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#Creates connections for the first time\n",
    "import time\n",
    "\n",
    "#list of the different key sets\n",
    "#\n",
    "# terminal_keys = Brazos_LowerColorado_ge5_values[4] \n",
    "# circular_keys = Brazos_LowerColorado_ge5_values[6]\n",
    "# terminal_keys_super = terminal_keys - circular_keys\n",
    "# con = Brazos_LowerColorado_ge5_values[0]\n",
    "# terminal_code = terminal_code_NHD\n",
    "# #\n",
    "# terminal_keys = CONUS_ge5_values[4] \n",
    "# circular_keys = CONUS_ge5_values[6]\n",
    "# terminal_keys_super = terminal_keys - circular_keys\n",
    "# con = CONUS_ge5_values[0]\n",
    "# #\n",
    "terminal_keys_super = test_terminal_keys - test_circular_keys\n",
    "con = test_connections\n",
    "terminal_code = test_terminal_code\n",
    "\n",
    "# terminal_keys = CONUS_FULL_RES_values[4] \n",
    "# circular_keys = CONUS_FULL_RES_values[6]\n",
    "# terminal_keys_super = terminal_keys - circular_keys\n",
    "# con = CONUS_FULL_RES_values[0]\n",
    "# terminal_code = terminal_code_NHD\n",
    "#orders of everything for computation in dictionary format\n",
    "order_dict={}\n",
    "junc_dict={}\n",
    "head_dict={}\n",
    "terminal_avg = {}\n",
    "\n",
    "def recursive_junction_read (keys, iterator, con, network, terminal_code, verbose = False, debuglevel = 0):\n",
    "    # print(keys)\n",
    "    for key in keys:        \n",
    "        result = 'node' + str(key) \n",
    "        order_dict[result] = iterator, nid\n",
    "        ckey = key\n",
    "        ukeys = con[key]['upstreams']\n",
    "        # terminal key and node_order assignment \n",
    "        con[key].update({'nid': nid})\n",
    "        con[key].update({'node_order' : iterator})\n",
    "        n = []\n",
    "        while not len(ukeys) >= 2 and not (ukeys == {terminal_code}):\n",
    "            # the terminal code will indicate a headwater\n",
    "            if debuglevel <= -2: print(ukeys)\n",
    "            (ckey,) = ukeys\n",
    "            ukeys = con[ckey]['upstreams']\n",
    "            network['segment_count'] += 1\n",
    "            #adds ordering for all nodes\n",
    "            result = 'node' + str(ckey)\n",
    "            order_dict[result] = iterator, nid\n",
    "            #assignment of terminal key and node order adjustment for continuous serial orders without junctions\n",
    "            con[ckey].update({'nid': nid})\n",
    "            con[ckey].update({'node_order' : iterator+1+sum(n)})\n",
    "            n.append(1)\n",
    "            \n",
    "        if len(ukeys) >= 2:\n",
    "            if debuglevel <= -1: print(f\"junction found at {ckey} with upstreams {ukeys}\")\n",
    "            network['segment_count'] += 1\n",
    "\n",
    "            network['junction_count'] += 1 #the Terminal Segment\n",
    "            #iterator adds 1 each iteration to provide a new order of computation for each junction section not each node or group of segments\n",
    "            result_junc = 'junc' + str(key)\n",
    "            junc_dict[result_junc] = iterator\n",
    "            con[ckey].update({'nid': nid})\n",
    "            \n",
    "            recursive_junction_read (ukeys, iterator+1+sum(n), con, network, terminal_code, verbose, debuglevel)\n",
    "            n.clear()\n",
    "        elif ukeys == {terminal_code}:\n",
    "            # print(f\"headwater found at {ckey}\")\n",
    "            network['segment_count'] += 1\n",
    "            #below adds headwaters to the headwater list\n",
    "            result_head = 'head' + str(key)\n",
    "            head_dict[result_head] = iterator\n",
    "            con[ckey].update({'nid': nid})\n",
    "            \n",
    "      \n",
    "def super_network_trace(nid, iterator, con, network, terminal_code, debuglevel = 0):\n",
    "    # print(f'\\ntraversing upstream on network {nid}:')\n",
    "    try:\n",
    "        network.update({'junction_count': 0})\n",
    "        network.update({'segment_count': 0}) #the Terminal Segment\n",
    "        \n",
    "        recursive_junction_read([nid], iterator , con, network, terminal_code, debuglevel = debuglevel)\n",
    "        \n",
    "    except Exception as exc:\n",
    "        print(exc)\n",
    "\n",
    "super_networks = {terminal_key:{}\n",
    "                        for terminal_key in terminal_keys_super}\n",
    "debuglevel = 0\n",
    "\n",
    "start_time = time.time()\n",
    "for nid, network in super_networks.items():\n",
    "    super_network_trace(nid, 0, con, network, terminal_code, debuglevel = debuglevel)\n",
    "    \n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Lengths Working\n",
    "- Adjustsments can be made to reaches too small first by adding lengths to upstream or downstream nodes \n",
    "- Adjustsments are then made to reaches too large by slicing them into the number of reaches that satisfies max length\n",
    "- New reaches are created and con values are updated to reflect this\n",
    "- New reaches are given max+1 ID values to avoid any duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'downstream': None, 'length': 850, 'upstreams': {3, 4, 6, 7}, 'nid': 0}\n",
      "1 {'downstream': 4, 'length': 678, 'upstreams': {-999}, 'nid': 0, 'node_order': 2}\n",
      "3 {'downstream': 0, 'length': 815, 'upstreams': {-999}, 'nid': 0}\n",
      "4 {'downstream': 0, 'length': 798, 'upstreams': {1, 5}, 'nid': 0}\n",
      "5 {'downstream': 4, 'length': 679, 'upstreams': {-999}, 'nid': 0, 'node_order': 2}\n",
      "6 {'downstream': 0, 'length': 394, 'upstreams': {-999}, 'nid': 0}\n",
      "7 {'downstream': 0, 'length': 815, 'upstreams': {-999}, 'nid': 0}\n",
      "8 {'downstream': None, 'length': 841, 'upstreams': {11, 12}, 'nid': 8, 'node_order': 0}\n",
      "9 {'downstream': 12, 'length': 524, 'upstreams': {10}, 'nid': 8, 'node_order': 2}\n",
      "10 {'downstream': 9, 'length': 458, 'upstreams': {14}, 'nid': 8, 'node_order': 3}\n",
      "11 {'downstream': 8, 'length': 548, 'upstreams': {-999}, 'nid': 8, 'node_order': 1}\n",
      "12 {'downstream': 8, 'length': 543, 'upstreams': {9}, 'nid': 8, 'node_order': 1}\n",
      "13 {'downstream': 14, 'length': 458, 'upstreams': {-999}, 'nid': 8, 'node_order': 5}\n",
      "14 {'downstream': 10, 'length': 548, 'upstreams': {13, 15}, 'nid': 8, 'node_order': 4}\n",
      "15 {'downstream': 14, 'length': 543, 'upstreams': {-999}, 'nid': 8, 'node_order': 5}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "new_con = con.copy()\n",
    "def grow(p):\n",
    "    for x , y in p.items():\n",
    "        #specify min length of reaches\n",
    "        if y['length'] < 400:\n",
    "          \n",
    "# for upstream junction with 1 downstream\n",
    "# updates up and downstream connections\n",
    "            if len(list(y['upstreams'])) > 1 and len(([y['downstream']])) == 1:\n",
    "                for w , q in p.items():\n",
    "                    if y['downstream'] == w:\n",
    "                        if x in (q['upstreams'].union(y['upstreams'])):\n",
    "                            i = (q['upstreams'].union(y['upstreams']))\n",
    "                            i.remove(x)\n",
    "                        else:\n",
    "                            i = (q['upstreams'].union(y['upstreams']))\n",
    "                        new_con.update({ w : {'downstream' : q['downstream'], 'length' : (q['length']+y['length']), 'upstreams' : i, 'nid': q['nid']}})\n",
    "                    for e in list(i): \n",
    "                        if w == e:\n",
    "                            new_con.update({ w : {'downstream' : y['downstream'], 'length' : q['length'], 'upstreams' : q['upstreams'], 'nid': q['nid']}})\n",
    "                         #downstream\n",
    "                del new_con[x]       \n",
    "\n",
    "#for 1 upstream with 1 downstream and upstream is not a headwater \n",
    "            if len(([y['downstream']])) == 1 and len(list(y['upstreams'])) == 1 and (y['upstreams']) != {-999}:\n",
    "                for d in list(y['upstreams']):\n",
    "                    for f , g in p.items():\n",
    "                        if d == f:\n",
    "                            if x in list(g['upstreams']):\n",
    "                                o = (g['upstreams'])\n",
    "                                o.remove(x)\n",
    "                            else:\n",
    "                                o = (g['upstreams'])\n",
    "                            new_con.update({ d : {'downstream' : y['downstream'], 'length' : ((g['length'])+y['length']), 'upstreams' : o, 'nid': g['nid']}})         \n",
    "                del new_con[x]\n",
    "       \n",
    "grow(con)\n",
    "for x,y in new_con.items():\n",
    "    print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'downstream': None, 'length': 425, 'upstreams': {16}, 'nid': 0}\n",
      "1 {'downstream': 4, 'length': 678, 'upstreams': {-999}, 'nid': 0, 'node_order': 2}\n",
      "3 {'downstream': 0, 'length': 407, 'upstreams': {17}, 'nid': 0}\n",
      "4 {'downstream': 0, 'length': 798, 'upstreams': {1, 5}, 'nid': 0}\n",
      "5 {'downstream': 4, 'length': 679, 'upstreams': {-999}, 'nid': 0, 'node_order': 2}\n",
      "6 {'downstream': 0, 'length': 394, 'upstreams': {-999}, 'nid': 0}\n",
      "7 {'downstream': 0, 'length': 407, 'upstreams': {18}, 'nid': 0}\n",
      "8 {'downstream': None, 'length': 420, 'upstreams': {19}, 'nid': 8}\n",
      "9 {'downstream': 12, 'length': 524, 'upstreams': {10}, 'nid': 8, 'node_order': 2}\n",
      "10 {'downstream': 9, 'length': 458, 'upstreams': {14}, 'nid': 8, 'node_order': 3}\n",
      "11 {'downstream': 8, 'length': 548, 'upstreams': {-999}, 'nid': 8, 'node_order': 1}\n",
      "12 {'downstream': 8, 'length': 543, 'upstreams': {9}, 'nid': 8, 'node_order': 1}\n",
      "13 {'downstream': 14, 'length': 458, 'upstreams': {-999}, 'nid': 8, 'node_order': 5}\n",
      "14 {'downstream': 10, 'length': 548, 'upstreams': {13, 15}, 'nid': 8, 'node_order': 4}\n",
      "15 {'downstream': 14, 'length': 543, 'upstreams': {-999}, 'nid': 8, 'node_order': 5}\n",
      "16 {'downstream': 0, 'length': 425, 'upstreams': {3, 4, 6, 7}, 'nid': 0}\n",
      "17 {'downstream': 3, 'length': 407, 'upstreams': {-999}, 'nid': 0}\n",
      "18 {'downstream': 7, 'length': 407, 'upstreams': {-999}, 'nid': 0}\n",
      "19 {'downstream': 8, 'length': 420, 'upstreams': {11, 12}, 'nid': 8}\n"
     ]
    }
   ],
   "source": [
    "#This creates new reaches by dividing up reaches that are too large\n",
    "import math\n",
    "con = new_con.copy()\n",
    "def shrink(p):\n",
    "    for x , y in p.items():\n",
    "        #Specify max length\n",
    "        if y['length'] > 800:\n",
    "            div = math.ceil(y['length']/800)\n",
    "            count = [int(div)]\n",
    "            n = div-1\n",
    "            temp = {(max(con.keys())+1)}\n",
    "            if sum(count) > 0:\n",
    "                con.update({x : {'downstream' : y['downstream'], 'length' : int(y['length']/div), 'upstreams' : temp, 'nid': y['nid']}}) #downstream       \n",
    "                for f in range(1,div):\n",
    "                    if f == 1 and f != div-1:\n",
    "                        con.update({(max(con.keys())+f) :{'downstream' : x, 'length' : int(y['length']/div), 'upstreams' :  {(max(con.keys())+2)}, 'nid': y['nid'] }}) # first node if only new node\n",
    "                    if  f == div-1:\n",
    "                        con.update({(max(con.keys())+f) :{'downstream' : x, 'length' : int(y['length']/div), 'upstreams' : y['upstreams'], 'nid': y['nid'] }}) #last node \n",
    "                     #new nodes first\n",
    "                    if f != 1 and f != div-1:\n",
    "                        con.update({(max(con.keys())+1) :{'downstream' : (max(con.keys())), 'length' : int(y['length']/div), 'upstreams' :{(max(con.keys())+2)}, 'nid': y['nid'] }}) #middle nodes\n",
    "                count.append(-1)\n",
    "            else:\n",
    "                print(\"done\")   \n",
    "shrink(new_con)\n",
    "for x,y in con.items():\n",
    "    print(x,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive network builder with ordering\n",
    "**Recursive functions capable of constructing the network from their given terminal, upstream, and downstream keys. **\n",
    "*   Segments between nodes are tallied.\n",
    "*   Junctions are outputted in junc_dict.\n",
    "*   Each node is assigned a computational order for parallel processing assigned to node_order.\n",
    "*   Inputted network keys (CONUS,BRAZOS,TEST) can be controlled under imports. \n",
    "*   IDs are passed to these functions from super_networks.items() to step through from the initial river outlets to the headwaters while labeling each order. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0001544952392578125 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#original \n",
    "import time\n",
    "\n",
    "#list of the different key sets\n",
    "#\n",
    "# terminal_keys = Brazos_LowerColorado_ge5_values[4] \n",
    "# circular_keys = Brazos_LowerColorado_ge5_values[6]\n",
    "# terminal_keys_super = terminal_keys - circular_keys\n",
    "# con = Brazos_LowerColorado_ge5_values[0]\n",
    "# terminal_code = terminal_code_NHD\n",
    "# #\n",
    "# terminal_keys = CONUS_ge5_values[4] \n",
    "# circular_keys = CONUS_ge5_values[6]\n",
    "# terminal_keys_super = terminal_keys - circular_keys\n",
    "# con = CONUS_ge5_values[0]\n",
    "# #\n",
    "# terminal_keys_super = test_terminal_keys - test_circular_keys\n",
    "# # con = test_connections\n",
    "# terminal_code = test_terminal_code\n",
    "\n",
    "# terminal_keys = CONUS_FULL_RES_values[4] \n",
    "# circular_keys = CONUS_FULL_RES_values[6]\n",
    "# terminal_keys_super = terminal_keys - circular_keys\n",
    "# con = CONUS_FULL_RES_values[0]\n",
    "# terminal_code = terminal_code_NHD\n",
    "#orders of everything for computation in dictionary format\n",
    "order_dict={}\n",
    "junc_dict={}\n",
    "head_dict={}\n",
    "terminal_avg = {}\n",
    "\n",
    "def recursive_junction_read (keys, iterator, con, network, terminal_code, verbose = False, debuglevel = 0):\n",
    "    # print(keys)\n",
    "    for key in keys:        \n",
    "        result = 'node' + str(key) \n",
    "        order_dict[result] = iterator, nid\n",
    "        ckey = key\n",
    "        ukeys = con[key]['upstreams']\n",
    "        # terminal key and node_order assignment \n",
    "        con[key].update({'nid': nid})\n",
    "        con[key].update({'node_order' : iterator})\n",
    "        n = []\n",
    "        while not len(ukeys) >= 2 and not (ukeys == {terminal_code}):\n",
    "            # the terminal code will indicate a headwater\n",
    "            if debuglevel <= -2: print(ukeys)\n",
    "            (ckey,) = ukeys\n",
    "            ukeys = con[ckey]['upstreams']\n",
    "            network['segment_count'] += 1\n",
    "            #adds ordering for all nodes\n",
    "            result = 'node' + str(ckey)\n",
    "            order_dict[result] = iterator, nid\n",
    "            #assignment of terminal key and node order adjustment for continuous serial orders without junctions\n",
    "            con[ckey].update({'nid': nid})\n",
    "            con[ckey].update({'node_order' : iterator+1+sum(n)})\n",
    "            n.append(1)\n",
    "            \n",
    "        if len(ukeys) >= 2:\n",
    "            if debuglevel <= -1: print(f\"junction found at {ckey} with upstreams {ukeys}\")\n",
    "            network['segment_count'] += 1\n",
    "\n",
    "            network['junction_count'] += 1 #the Terminal Segment\n",
    "            #iterator adds 1 each iteration to provide a new order of computation for each junction section not each node or group of segments\n",
    "            result_junc = 'junc' + str(key)\n",
    "            junc_dict[result_junc] = iterator\n",
    "            con[ckey].update({'nid': nid})\n",
    "            \n",
    "            recursive_junction_read (ukeys, iterator+1+sum(n), con, network, terminal_code, verbose, debuglevel)\n",
    "            n.clear()\n",
    "        elif ukeys == {terminal_code}:\n",
    "            # print(f\"headwater found at {ckey}\")\n",
    "            network['segment_count'] += 1\n",
    "            #below adds headwaters to the headwater list\n",
    "            result_head = 'head' + str(key)\n",
    "            head_dict[result_head] = iterator\n",
    "            con[ckey].update({'nid': nid})\n",
    "            \n",
    "      \n",
    "def super_network_trace(nid, iterator, con, network, terminal_code, debuglevel = 0):\n",
    "    # print(f'\\ntraversing upstream on network {nid}:')\n",
    "    try:\n",
    "        network.update({'junction_count': 0})\n",
    "        network.update({'segment_count': 0}) #the Terminal Segment\n",
    "        \n",
    "        recursive_junction_read([nid], iterator , con, network, terminal_code, debuglevel = debuglevel)\n",
    "        \n",
    "    except Exception as exc:\n",
    "        print(exc)\n",
    "\n",
    "super_networks = {terminal_key:{}\n",
    "                        for terminal_key in terminal_keys_super}\n",
    "debuglevel = 0\n",
    "\n",
    "start_time = time.time()\n",
    "for nid, network in super_networks.items():\n",
    "    super_network_trace(nid, 0, con, network, terminal_code, debuglevel = debuglevel)\n",
    "    \n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# for x,y in con.items():\n",
    "#     print(x,y)\n",
    "# print(super_networks.items())\n",
    "# print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'downstream': None, 'length': 425, 'upstreams': {16}, 'nid': 0, 'node_order': 0}\n",
      "1 {'downstream': 4, 'length': 678, 'upstreams': {-999}, 'nid': 0, 'node_order': 3}\n",
      "3 {'downstream': 0, 'length': 407, 'upstreams': {17}, 'nid': 0, 'node_order': 2}\n",
      "4 {'downstream': 0, 'length': 798, 'upstreams': {1, 5}, 'nid': 0, 'node_order': 2}\n",
      "5 {'downstream': 4, 'length': 679, 'upstreams': {-999}, 'nid': 0, 'node_order': 3}\n",
      "6 {'downstream': 0, 'length': 394, 'upstreams': {-999}, 'nid': 0, 'node_order': 2}\n",
      "7 {'downstream': 0, 'length': 407, 'upstreams': {18}, 'nid': 0, 'node_order': 2}\n",
      "8 {'downstream': None, 'length': 420, 'upstreams': {19}, 'nid': 8, 'node_order': 0}\n",
      "9 {'downstream': 12, 'length': 524, 'upstreams': {10}, 'nid': 8, 'node_order': 3}\n",
      "10 {'downstream': 9, 'length': 458, 'upstreams': {14}, 'nid': 8, 'node_order': 4}\n",
      "11 {'downstream': 8, 'length': 548, 'upstreams': {-999}, 'nid': 8, 'node_order': 2}\n",
      "12 {'downstream': 8, 'length': 543, 'upstreams': {9}, 'nid': 8, 'node_order': 2}\n",
      "13 {'downstream': 14, 'length': 458, 'upstreams': {-999}, 'nid': 8, 'node_order': 6}\n",
      "14 {'downstream': 10, 'length': 548, 'upstreams': {13, 15}, 'nid': 8, 'node_order': 5}\n",
      "15 {'downstream': 14, 'length': 543, 'upstreams': {-999}, 'nid': 8, 'node_order': 6}\n",
      "16 {'downstream': 0, 'length': 425, 'upstreams': {3, 4, 6, 7}, 'nid': 0, 'node_order': 1}\n",
      "17 {'downstream': 3, 'length': 407, 'upstreams': {-999}, 'nid': 0, 'node_order': 3}\n",
      "18 {'downstream': 7, 'length': 407, 'upstreams': {-999}, 'nid': 0, 'node_order': 3}\n",
      "19 {'downstream': 8, 'length': 420, 'upstreams': {11, 12}, 'nid': 8, 'node_order': 1}\n"
     ]
    }
   ],
   "source": [
    "for x,y in con.items():\n",
    "    print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
