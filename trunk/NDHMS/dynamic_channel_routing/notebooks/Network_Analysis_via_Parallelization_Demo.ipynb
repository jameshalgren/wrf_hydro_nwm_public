{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "riA1WwTTID3q"
   },
   "source": [
    "## NHD Network Analysis Demo\n",
    "___TL;DR___: *We are trying to parallelize hydraulic calculations for dynamic subsets of the U.S. river and stream network*<br><br>\n",
    "The following was developed as part of the process of preparing a method for forecasting flows on the US network of rivers and streams as represented in the National Hydrography Dataset (NHD). The NHD is a continuously evolving characterization of a fractal system so we felt that we needed to plan to have some flexibility. We hope to identify the complexity inherent in the network at different levels of resolution and we hope to be able to do so dynamically. The goal is also to be able to manage the complexity calculation for arbitrary collections of headwater points, such as might be obtained from a list of named streams or during a major flood event in a particular region.<br>\n",
    "As a point of terminology, we use the word 'routing' as shorthand to refer to the computation of the translation of a particular flow condition, high or low, to downstream (or in some cases upstream) areas of influence.\n",
    "The network complexity is related to the potential for parallelization of a serial analysis of the network. We have identified three levels of parallelization that may be implemented: \n",
    "1. System-level parallelization of independent networks -- the routing computations for the Mississippi River have little (nothing, except conceptual similarity and a shared existence on earth) to do with the computations for the Columbia river for any practical level of analysis. The system of networks across the US is what we are considering in general.\n",
    "1. Network-level parallelization of interconnected reaches -- There is a need to consider the computations for adjacent branches within a network of con-flowing streams, but with proper ordering, some of the computations may be considered in parallel. For example, the Illinois River headwaters and the Mississippi River headwaters are related within their broader Mississippi network, but the routing calculations for those headwaters are pratically agnostic to one another.\n",
    "1. Reach-level parallelization of the specific routing computation -- the numerical work of routing water downstream is a matrix computation and consists of exploring solutions to differential equations, all of which may potentially be examined in parallel, under the proper conditions and with suitable assumptions.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qv_SVt3VzC9J"
   },
   "source": [
    "### Import the git repo including test data\n",
    "This git repo is a fork/branch of the national water model public repository hosted by UCAR. The UCAR repo is the basis for the WRF-Hydro model that is presently the modeling engine of the [US National Water Model.](https://water.noaa.gov/about/nwm)<br>\n",
    "\n",
    "The network analysis code assumes that the downstream neighbor is identified in the table for each stream segment as is the case for the test datasets. \n",
    "\n",
    "We recognize that others have done similar work and may possibly have done it better. We are working on being more able to nimbly respond to suggestions and opportunities for improvement. Please let us know if you see something we could do better or of course feel free to fork and improve what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "0RBaWmyY7Z88",
    "outputId": "5e88d24d-9547-4347-eb77-509384696c2b"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "try:\n",
    "    import google.colab\n",
    "    ENV_IS_CL = True\n",
    "    !git clone --single-branch --branch dynamic_channel_routing https://github.com/jameshalgren/wrf_hydro_nwm_public.git\n",
    "    sys.path.append('/content/wrf_hydro_nwm_public/trunk/NDHMS/dynamic_channel_routing/src/python_framework')\n",
    "    !pip install geopandas\n",
    "    !pip install netcdf4\n",
    "    #default recursion limit (~1000) is slightly too small for the deepest branches of the network\n",
    "    sys.setrecursionlimit(6000) \n",
    "    #TODO: convert recursive functions to stack-based functions\n",
    "except:\n",
    "    ENV_IS_CL = False\n",
    "    sys.path.append(r'../src/python_framework')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mJ0UirjC0iN1"
   },
   "source": [
    "### Create some general functions\n",
    "The next three blocks define interaction with the `networkbuilder` module in the git repo, which is the tool for creating the `connnections` object to characterize the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "keRYtaWBrgnz"
   },
   "outputs": [],
   "source": [
    "import networkbuilder as networkbuilder\n",
    "import recursive_print\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"NHD Network traversal\n",
    "\n",
    "A demonstration version of this code is stored in this Colaboratory notebook:\n",
    "https://colab.research.google.com/github/jameshalgren/wrf_hydro_nwm_public/blob/network/trunk/NDHMS/dynamic_channel_routing/notebooks/NHD_Network_Density_Analysis.ipynb#scrollTo=h_BEdl4LID34\n",
    "\n",
    "\"\"\"\n",
    "def do_network(\n",
    "        geofile_path = None\n",
    "        , title_string = None\n",
    "        , layer_string = None\n",
    "        , driver_string = None\n",
    "        , key_col = None\n",
    "        , downstream_col = None\n",
    "        , length_col = None\n",
    "        , terminal_code = None\n",
    "        , verbose = False\n",
    "        , debuglevel = 0\n",
    "        ):\n",
    "\n",
    "    # NOTE: these methods can lose the \"connections\" and \"rows\" arguments when\n",
    "    # implemented as class methods where those arrays are members of the class.\n",
    "    if verbose: print(title_string)\n",
    "    if driver_string == 'NetCDF':\n",
    "        geofile = xr.open_dataset(geofile_path)\n",
    "        geofile_rows = (geofile.to_dataframe()).values\n",
    "        # The xarray method for NetCDFs was implemented after the geopandas method for \n",
    "        # GIS source files. It's possible (probable?) that we are doing something \n",
    "        # inefficient by converting away from the Pandas dataframe.\n",
    "        # TODO: Check the optimal use of the Pandas dataframe\n",
    "        if debuglevel <= -1: print(f'reading -- dataset: {geofile_path}; layer: {layer_string}; driver: {driver_string}')\n",
    "    else:\n",
    "        if debuglevel <= -1: print(f'reading -- dataset: {geofile_path}; layer: {layer_string}; fiona driver: {driver_string}')\n",
    "        geofile = gpd.read_file(geofile_path, driver=driver_string, layer=layer_string)\n",
    "        geofile_rows = geofile.to_numpy()\n",
    "    if debuglevel <= -2: geofile.plot() #TODO: WILL THIS WORK WITH NetCDF???\n",
    "    if debuglevel <= -1: print(geofile.head()) #TODO: WILL THIS WORK WITH NetCDF???\n",
    "    # Kick off recursive call for all connections and keys\n",
    "    (connections) = networkbuilder.get_down_connections(\n",
    "                    rows = geofile_rows\n",
    "                    , key_col = key_col\n",
    "                    , downstream_col = downstream_col\n",
    "                    , length_col = length_col\n",
    "                    , verbose = verbose\n",
    "                    , debuglevel = debuglevel)\n",
    "    \n",
    "    (all_keys, ref_keys, headwater_keys\n",
    "        , terminal_keys\n",
    "        , terminal_ref_keys\n",
    "        , circular_keys) = networkbuilder.determine_keys(\n",
    "                    connections = connections\n",
    "#                     , rows = geofile_rows\n",
    "                    , key_col = key_col\n",
    "                    , downstream_col = downstream_col\n",
    "                    , terminal_code = terminal_code\n",
    "                    , verbose = verbose\n",
    "                    , debuglevel = debuglevel)\n",
    "    \n",
    "    (junction_keys) = networkbuilder.get_up_connections(\n",
    "                    connections = connections\n",
    "                    , terminal_code = terminal_code\n",
    "                    , headwater_keys = headwater_keys\n",
    "                    , terminal_keys = terminal_keys\n",
    "                    , verbose = verbose\n",
    "                    , debuglevel = debuglevel)\n",
    "    return connections, all_keys, ref_keys, headwater_keys \\\n",
    "        , terminal_keys, terminal_ref_keys \\\n",
    "        , circular_keys, junction_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTYCYXeVrMCo"
   },
   "outputs": [],
   "source": [
    "def do_print():    \n",
    "    recursive_print.print_basic_network_info(\n",
    "                    connections = connections_NHD\n",
    "                    , headwater_keys = headwater_keys_NHD\n",
    "                    , junction_keys = junction_keys_NHD\n",
    "                    , terminal_keys = terminal_keys_NHD\n",
    "                    , terminal_code = terminal_code_NHD\n",
    "                    , verbose = True\n",
    "                    )\n",
    "    \n",
    "    if 1 == 0: #THE RECURSIVE PRINT IS NOT A GOOD IDEA WITH LARGE NETWORKS!!!\n",
    "        recursive_print.print_connections(\n",
    "                    headwater_keys = headwater_keys_NHD\n",
    "                    , down_connections = connections_NHD\n",
    "                    , up_connections = connections_NHD\n",
    "                    , terminal_code = terminal_code_NHD\n",
    "                    , terminal_keys = terminal_keys_NHD\n",
    "                    , terminal_ref_keys = terminal_ref_keys_NHD\n",
    "                    , debuglevel = -2\n",
    "                    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "diFflaIMsef-"
   },
   "source": [
    "### Build a test case\n",
    "The `test_rows` object simulates a river network dataset such as we recieve from the National Hydrography Dataset. Each data row has a node ID, a 'to' node ID, and some other relevant data. For this test dataset, the second data column is a dummy length (and the last column could be some other value, but we haven't tried anything yet... stay tuned) and in our traversals, we can add up the lengths as a surrogate for more complex water routing functions we need to eventually manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "KFnY-vZDYPue",
    "outputId": "58c2d71e-59d2-4d4e-818b-2dea1a337471"
   },
   "outputs": [],
   "source": [
    "# def main():\n",
    "if 1 == 1:\n",
    "    \"\"\"##TEST\"\"\"\n",
    "    print(\"\")\n",
    "    print ('Executing Test')\n",
    "    # Test data\n",
    "    test_rows = [\n",
    "        [50,178,51,0],\n",
    "        [51,178,50,0],\n",
    "        [60,178,61,0],\n",
    "        [61,178,62,0],\n",
    "        [62,178,60,0],\n",
    "        [70,178,71,0],\n",
    "        [71,178,72,0],\n",
    "        [72,178,73,0],\n",
    "        [73,178,70,0],\n",
    "        [80,178,81,0],\n",
    "        [81,178,82,0],\n",
    "        [82,178,83,0],\n",
    "        [83,178,84,0],\n",
    "        [84,178,80,0],\n",
    "        [0,456,-999,0],\n",
    "        [1,178,4,0],\n",
    "        [2,394,0,0],\n",
    "        [3,301,2,0],\n",
    "        [4,798,0,0],\n",
    "        [5,679,4,0],\n",
    "        [6,523,0,0],\n",
    "        [7,815,2,0],\n",
    "        [8,841,-999,0],\n",
    "        [9,514,8,0],\n",
    "        [10,458,9,0],\n",
    "        [11,832,10,0],\n",
    "        [12,543,11,0],\n",
    "        [13,240,12,0],\n",
    "        [14,548,13,0],\n",
    "        [15,920,14,0],\n",
    "        [16,920,15,0],\n",
    "        [17,514,16,0],\n",
    "        [18,458,17,0],\n",
    "        [19,832,18,0],\n",
    "        [20,543,19,0],\n",
    "        [21,240,16,0],\n",
    "        [22,548,21,0],\n",
    "        [23,920,22,0],\n",
    "        [24,240,23,0],\n",
    "        [25,548,12,0],\n",
    "        [26,920,25,0],\n",
    "        [27,920,26,0],\n",
    "        [28,920,27,0],\n",
    "    ]\n",
    "\n",
    "    test_key_col = 0\n",
    "    test_downstream_col = 2\n",
    "    test_length_col = 1\n",
    "    test_terminal_code = -999\n",
    "    debuglevel = 0\n",
    "    verbose = True\n",
    "\n",
    "    (test_connections) = networkbuilder.get_down_connections(\n",
    "                rows = test_rows\n",
    "                , key_col = test_key_col\n",
    "                , downstream_col = test_downstream_col\n",
    "                , length_col = test_length_col\n",
    "                , verbose = True\n",
    "                , debuglevel = debuglevel\n",
    "                )\n",
    "\n",
    "    (test_all_keys, test_ref_keys, test_headwater_keys\n",
    "     , test_terminal_keys\n",
    "     , test_terminal_ref_keys\n",
    "     , test_circular_keys) = networkbuilder.determine_keys(\n",
    "                connections = test_connections\n",
    "                , key_col = test_key_col\n",
    "                , downstream_col = test_downstream_col\n",
    "                , terminal_code = test_terminal_code\n",
    "                , verbose = True\n",
    "                , debuglevel = debuglevel\n",
    "                )\n",
    "\n",
    "    test_junction_keys = networkbuilder.get_up_connections(\n",
    "                connections = test_connections\n",
    "                , terminal_code = test_terminal_code\n",
    "                , headwater_keys = test_headwater_keys\n",
    "                , terminal_keys = test_terminal_keys\n",
    "                , verbose = True\n",
    "                , debuglevel = debuglevel\n",
    "                )\n",
    "\n",
    "    recursive_print.print_connections(\n",
    "                headwater_keys = test_headwater_keys\n",
    "                , down_connections = test_connections\n",
    "                , up_connections = test_connections\n",
    "                , terminal_code = test_terminal_code\n",
    "                , terminal_keys = test_terminal_keys\n",
    "                , terminal_ref_keys = test_terminal_ref_keys\n",
    "                , debuglevel = debuglevel\n",
    "                )\n",
    "\n",
    "    recursive_print.print_basic_network_info(\n",
    "                connections = test_connections\n",
    "                , headwater_keys = test_headwater_keys\n",
    "                , junction_keys = test_junction_keys\n",
    "                , terminal_keys = test_terminal_keys\n",
    "                , terminal_code = test_terminal_code\n",
    "                , verbose = True\n",
    "                , debuglevel = debuglevel\n",
    "                )\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5xhb-Q4pCbX"
   },
   "source": [
    "### Real Networks\n",
    "(you can skip this cell to test the code on the simple case generated above...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "il50SP8eZBk2",
    "outputId": "32c43141-2c53-4919-b1e6-a81dd9f6dc92"
   },
   "outputs": [],
   "source": [
    "if ENV_IS_CL: root = '/content/wrf_hydro_nwm_public/trunk/NDHMS/dynamic_channel_routing/'\n",
    "elif not ENV_IS_CL: root = os.path.dirname(os.path.abspath(''))\n",
    "test_folder = os.path.join(root, r'test')\n",
    "geo_input_folder = os.path.join(test_folder, r'input', r'geo', r'Channels')\n",
    "\n",
    "\"\"\"##NHD Subset (Brazos/Lower Colorado)\"\"\"\n",
    "Brazos_LowerColorado_ge5 = True\n",
    "\"\"\"##NHD CONUS order 5 and greater\"\"\"\n",
    "CONUS_ge5 = True\n",
    "\"\"\"These are large -- be careful\"\"\"\n",
    "CONUS_FULL_RES = True\n",
    "CONUS_Named_Streams = False #create a subset of the full resolution by reading the GNIS field\n",
    "CONUS_Named_combined = False #process the Named streams through the Full-Res paths to join the many hanging reaches\n",
    "\n",
    "debuglevel = -1\n",
    "verbose = True\n",
    "\n",
    "# The CONUS_ge5 and Brazos_LowerColorado_ge5 datasets are included\n",
    "# in the github test folder and are extracts from the NHD version 1.2 datasets\n",
    "# from https://www.nohrsc.noaa.gov/pub/staff/keicher/NWM_live/web/data_tools/\n",
    "#  \n",
    "# The CONUS_FULL_RES file was generated from the RouteLink file in the parameter\n",
    "# archive and converted to a compressed NetCDF via the following command:\n",
    "# nccopy -d1 -s RouteLink_NWMv2.0_20190517_cheyenne_pull.nc RouteLink_NWMv2.0_20190517_cheyenne_pull.nc4s\n",
    "# TODO: Explain CONUS_Named_Streams\n",
    "# CONUS_Named_Streams was generated by intersecting the FULL_RES file ...\n",
    "# of the data in the nohrsc-hosted archive but are too large to efficiently \n",
    "# package inside of the repository. \n",
    "\n",
    "if Brazos_LowerColorado_ge5:\n",
    "    nhd_conus_file_path = os.path.join(geo_input_folder\n",
    "            , r'NHD_BrazosLowerColorado_Channels.shp')\n",
    "    key_col_NHD = 2\n",
    "    downstream_col_NHD = 7\n",
    "    length_col_NHD = 6\n",
    "    terminal_code_NHD = 0\n",
    "    title_string = 'Brazos + Lower Colorado\\nNHD stream orders 5 and greater\\n'\n",
    "    title_string = 'CONUS Order 5 and Greater '\n",
    "    driver_string = 'ESRI Shapefile'\n",
    "    layer_string = 0\n",
    "\n",
    "    Brazos_LowerColorado_ge5_values = do_network (nhd_conus_file_path\n",
    "                , title_string = title_string\n",
    "                , layer_string = layer_string\n",
    "                , driver_string = driver_string\n",
    "                , key_col = key_col_NHD\n",
    "                , downstream_col = downstream_col_NHD\n",
    "                , length_col = length_col_NHD\n",
    "                , terminal_code = terminal_code_NHD\n",
    "                , verbose = verbose\n",
    "                , debuglevel = debuglevel)\n",
    "\n",
    "if CONUS_ge5:\n",
    "    nhd_conus_file_path = os.path.join(geo_input_folder\n",
    "            , r'NHD_Conus_Channels.shp')\n",
    "    key_col_NHD = 1\n",
    "    downstream_col_NHD = 6\n",
    "    length_col_NHD = 5\n",
    "    terminal_code_NHD = 0\n",
    "    title_string = 'CONUS Order 5 and Greater '\n",
    "    driver_string = 'ESRI Shapefile'\n",
    "    layer_string = 0\n",
    "\n",
    "    CONUS_ge5_values = do_network (nhd_conus_file_path\n",
    "                , title_string = title_string\n",
    "                , layer_string = layer_string\n",
    "                , driver_string = driver_string\n",
    "                , key_col = key_col_NHD\n",
    "                , downstream_col = downstream_col_NHD\n",
    "                , length_col = length_col_NHD\n",
    "                , terminal_code = terminal_code_NHD\n",
    "                , verbose = verbose\n",
    "                , debuglevel = debuglevel)\n",
    "\n",
    "if CONUS_FULL_RES:\n",
    "    # nhd_conus_file_path = '../../../../../../GISTemp/nwm_v12.gdb'\n",
    "    nhd_conus_file_path = os.path.join(geo_input_folder\n",
    "            , r'RouteLink_NWMv2.0_20190517_cheyenne_pull.nc')\n",
    "    key_col_NHD = 0\n",
    "    downstream_col_NHD = 2\n",
    "    length_col_NHD = 8\n",
    "    terminal_code_NHD = 0\n",
    "    title_string = 'CONUS Full Resolution NWM v2.0'\n",
    "    # driver_string = 'FileGDB'\n",
    "    driver_string = 'NetCDF'\n",
    "    # layer_string = 'channels_nwm_v12_routeLink'\n",
    "    layer_string = 0\n",
    "\n",
    "    CONUS_FULL_RES_values = do_network (nhd_conus_file_path\n",
    "                , title_string = title_string\n",
    "                , layer_string = layer_string\n",
    "                , driver_string = driver_string\n",
    "                , key_col = key_col_NHD\n",
    "                , downstream_col = downstream_col_NHD\n",
    "                , length_col = length_col_NHD\n",
    "                , terminal_code = terminal_code_NHD\n",
    "                , verbose = verbose\n",
    "                , debuglevel = debuglevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "il50SP8eZBk2",
    "outputId": "32c43141-2c53-4919-b1e6-a81dd9f6dc92"
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(networkbuilder)\n",
    "import pickle\n",
    "import zipfile # TODO: Incorporate Named streams as zip into test datasets\n",
    "\n",
    "# CONUS_Named_Streams = True # This variable is set in the previous cell\n",
    "# CONUS_Named_combined = True # This variable is set in the previous cell\n",
    "\n",
    "if CONUS_Named_Streams:\n",
    "    nhd_conus_file_path = os.path.join(geo_input_folder\n",
    "            , r'channels_nwm_v12_routeLink_NamedOnly.shp')\n",
    "    key_col_Named_Streams = 0\n",
    "    downstream_col_Named_Streams = 5\n",
    "    length_col_Named_Streams = 4\n",
    "    terminal_code_Named_Streams = 0\n",
    "    title_string = 'NHD v1.2 segments corresponding to NHD 2.0 GNIS labeled streams\\n'\n",
    "    # driver_string = 'FileGDB'\n",
    "    driver_string = 'ESRI Shapefile'\n",
    "    # layer_string = 'named_streams_v12'\n",
    "    layer_string = 0\n",
    "\n",
    "    CONUS_Named_Streams_values = do_network (nhd_conus_file_path\n",
    "                , title_string = title_string\n",
    "                , layer_string = layer_string\n",
    "                , driver_string = driver_string\n",
    "                , key_col = key_col_Named_Streams\n",
    "                , downstream_col = downstream_col_Named_Streams\n",
    "                , length_col = length_col_Named_Streams\n",
    "                , terminal_code = terminal_code_Named_Streams\n",
    "                , verbose = verbose\n",
    "                , debuglevel = debuglevel)\n",
    "\n",
    "\n",
    "if CONUS_Named_combined:\n",
    "    ''' NOW Combine the two CONUS analyses by starting with the Named Headwaters\n",
    "        but trace the network down the Full Resolution NHD. It should only work\n",
    "        if the other two datasets have been computed.\n",
    "        ANY OTHER Set of Headerwaters could be substituted'''\n",
    "    \n",
    "    if not (CONUS_Named_Streams and CONUS_FULL_RES):\n",
    "        print('\\n\\nWARNING: If this works, you are using old data...')\n",
    "\n",
    "    \n",
    "    # Use only headwater keys that are in the full dataset.\n",
    "    headwater_keys_combined = CONUS_FULL_RES_values[3] & \\\n",
    "                                CONUS_Named_Streams_values[3]\n",
    "    # Need to make sure that these objects are independent -- we will modify them a bit.\n",
    "    connections_combined = pickle.loads(pickle.dumps(CONUS_FULL_RES_values[0]))\n",
    "    terminal_keys_combined = pickle.loads(pickle.dumps(CONUS_FULL_RES_values[4]))\n",
    "    terminal_code_combined = terminal_code_NHD\n",
    "    \n",
    "    for key in connections_combined: #Clear the upstreams and rebuild it with just named streams\n",
    "        connections_combined[key].pop('upstreams',None)\n",
    "\n",
    "    (junction_keys_combined\n",
    "     , visited_keys_combined\n",
    "     , visited_terminal_keys_combined\n",
    "     , junction_count_combined) = networkbuilder.get_up_connections(\n",
    "                 connections = connections_combined\n",
    "                 , terminal_code = terminal_code_combined\n",
    "                 , headwater_keys = headwater_keys_combined\n",
    "                 , terminal_keys = terminal_keys_combined\n",
    "                 , verbose = verbose\n",
    "                 , debuglevel = debuglevel)\n",
    "    \n",
    "# # Useful for debugging the combined calculation\n",
    "#     print(len(junction_keys_combined)\n",
    "#      , len(visited_keys_combined)\n",
    "#      , len(visited_terminal_keys_combined)\n",
    "#      , junction_count_combined)\n",
    "    \n",
    "#     print(len(terminal_keys_combined - visited_terminal_keys_combined))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_fuRiEXFsvOK"
   },
   "source": [
    "### With this, we can separate the different rivers in the network\n",
    "Once a 'connection' object has been created with a representation of the river network, we can traverse that object and perform calculations -- in the example below, we parallelize the process of traversing the independent portions of the network and then serially compute the number of junctions. This corresponds to the \"**System-level parallelization**\" mentioned as _item 1_ above.\n",
    "### NOW for the next step\n",
    "We could compute total upstream length or (and this is the real goal) flow due to incoming lateral contributions from the land accumulated over the entire upstream network. That second calculation can also be parallelized but we have to figure out how to accomplish intelligently so that the collective calculation is network-aware. Such a parallelization would be the \"**Network-level parallelization of interconnected reaches**\" mentioned as _item 2_ above. The upstream length will depend on the number of upstream branches and their configuration, so there has to be some concept of stream order and topology built into the parallelization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#parallel compute\n",
    "import time\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "'''Test'''\n",
    "# terminal_code = test_terminal_code\n",
    "# terminal_keys = test_terminal_keys \n",
    "# circular_keys = test_circular_keys\n",
    "# terminal_keys_super = terminal_keys - circular_keys\n",
    "# con = test_connections\n",
    "'''Streams of NHD order 5 or greater confluent to the Brazos and Lower Colorado'''\n",
    "# terminal_code = terminal_code_NHD\n",
    "# terminal_keys = Brazos_LowerColorado_ge5_values[4] \n",
    "# circular_keys = Brazos_LowerColorado_ge5_values[6]\n",
    "# terminal_keys_super = terminal_keys - circular_keys\n",
    "# con = Brazos_LowerColorado_ge5_values[0]\n",
    "'''CONUS streams of NHD order 5 or greater'''\n",
    "# terminal_code = terminal_code_NHD\n",
    "# terminal_keys = CONUS_ge5_values[4] \n",
    "# circular_keys = CONUS_ge5_values[6]\n",
    "# terminal_keys_super = terminal_keys - circular_keys\n",
    "# con = CONUS_ge5_values[0]\n",
    "'''Named Streams'''\n",
    "# terminal_keys = CONUS_Named_Streams_values[4] \n",
    "# circular_keys = CONUS_Named_Streams_values[6]\n",
    "# terminal_keys_super = terminal_keys - circular_keys\n",
    "# con = CONUS_Named_Streams_values[0]\n",
    "# terminal_code = terminal_code_NHD\n",
    "'''Full Res NHD'''\n",
    "terminal_keys = CONUS_FULL_RES_values[4] \n",
    "circular_keys = CONUS_FULL_RES_values[6]\n",
    "terminal_keys_super = terminal_keys - circular_keys\n",
    "con = CONUS_FULL_RES_values[0]\n",
    "terminal_code = terminal_code_NHD\n",
    "'''Combined'''\n",
    "# # # NOT NEEDED # terminal_keys = terminal_keys_combined \n",
    "# # # NOT NEEDED # circular_keys = circular_keys_combined\n",
    "# terminal_keys_super = visited_terminal_keys_combined\n",
    "# con = connections_combined\n",
    "# terminal_code = terminal_code_NHD\n",
    "\n",
    "def recursive_junction_read (\n",
    "                             keys\n",
    "                             , network\n",
    "                             , terminal_code = 0\n",
    "                             , verbose = False\n",
    "                             , debuglevel = 0\n",
    "                            ):\n",
    "    global con\n",
    "    for key in keys:\n",
    "        ckey = key\n",
    "        try:\n",
    "            ukeys = con[key]['upstreams']\n",
    "            while not len(ukeys) >= 2 and not (ukeys == {terminal_code}):\n",
    "                if debuglevel <= -3: print(f\"segs at ckey {ckey}: {network['segment_count']}\")\n",
    "                # the terminal code will indicate a headwater\n",
    "                if debuglevel <= -4: print(ukeys)\n",
    "                (ckey,) = ukeys\n",
    "                ukeys = con[ckey]['upstreams']\n",
    "            if ukeys == {terminal_code}:\n",
    "                if debuglevel <= -3: print(f\"headwater found at {ckey}\")\n",
    "                network['segment_count'] += 1\n",
    "                if debuglevel <= -3: print(f\"segs at ckey {ckey}: {network['segment_count']}\")\n",
    "            elif len(ukeys) >= 2:\n",
    "                network['segment_count'] += 1\n",
    "                if debuglevel <= -3: print(f\"junction found at {ckey} with upstreams {ukeys}\")\n",
    "                network['segment_count'] += 1\n",
    "                if debuglevel <= -3: print(f\"segs at ckey {ckey}: {network['segment_count']}\")\n",
    "                network['junction_count'] += 1 #the Terminal Segment\n",
    "                recursive_junction_read (ukeys, network, terminal_code = terminal_code, verbose = verbose, debuglevel = debuglevel) \n",
    "                # print(ukeys)\n",
    "                ukeys = con[ckey]['upstreams']\n",
    "                ckey = ukeys\n",
    "        except:\n",
    "            if debuglevel <= -2: \n",
    "                print(f'There is a problem with connection: {key}: {con[key]}')\n",
    "\n",
    "def super_network_trace(\n",
    "                        nid\n",
    "                        , terminal_code = terminal_code\n",
    "                        , verbose= False\n",
    "                        , debuglevel = 0\n",
    "                        ):\n",
    "\n",
    "    network = {}\n",
    "    global con\n",
    "    us_length_total = 0\n",
    "    \n",
    "    if verbose: print(f'\\ntraversing upstream on network {nid}:')\n",
    "    # try:\n",
    "    if 1 == 1:\n",
    "        network.update({'junction_count': 0})\n",
    "        network.update({'segment_count': 0}) #the Terminal Segment\n",
    "        recursive_junction_read([nid], network, verbose = verbose, terminal_code = terminal_code, debuglevel = debuglevel)\n",
    "        if verbose: print(f\"junctions: {network['junction_count']}\")\n",
    "        if verbose: print(f\"segments: {network['segment_count']}\")\n",
    "    # except Exception as exc:\n",
    "    #     print(exc)\n",
    "    #TODO: compute upstream length as a surrogate for the routing computation\n",
    "    return {nid: network, 'upstream_length': us_length_total}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "Gz15rLB9WnBl",
    "outputId": "b872a326-258a-46f8-e9d6-66e0772a7957"
   },
   "outputs": [],
   "source": [
    "###continuing from previous cell\n",
    "super_networks = {terminal_key:{}\n",
    "                  for terminal_key in terminal_keys_super \n",
    "# # Toggle these comments to try removing/isolating some of the larger networks\n",
    "#                   if terminal_key in [\n",
    "#                   if terminal_key not in [\n",
    "#                           22811611 # Mississippi River, LA\n",
    "#                           , 23832907 # Columbia River, WA\n",
    "#                           , 626220 # Rio Grande, TX/MX\n",
    "#                           , 21412883 # Colorado River, AZ/MX\n",
    "#                           , 18524217 # Mobile River, AL\n",
    "#                           , 15183793 # Atchafalaya, LA\n",
    "#                   ]\n",
    "                 }  \n",
    "debuglevel = -1\n",
    "verbose = False\n",
    "\n",
    "if verbose: print('verbose output')\n",
    "if verbose: print(f'number of Independent Networks to be analyzed is {len(super_networks)}')\n",
    "if verbose: print(f'Multi-processing will use {multiprocessing.cpu_count()} CPUs')\n",
    "if verbose: print(f'debuglevel is {debuglevel}')\n",
    "\n",
    "start_time = time.time()\n",
    "results_serial = {}\n",
    "for nid, network in super_networks.items():\n",
    "    network.update(super_network_trace(nid, terminal_code = terminal_code, verbose = verbose, debuglevel = debuglevel)[nid])\n",
    "print(\"--- %s seconds: serial compute ---\" % (time.time() - start_time))\n",
    "if debuglevel <= -1: print(len(super_networks.items()))\n",
    "if debuglevel <= -2: print(super_networks)\n",
    "\n",
    "    ## Notice that I'm not timing the initialization in each case, \n",
    "    ## which might be considered cheating a little bit. \n",
    "    ## I timed it for the first case for reference.\n",
    "nids = (nid for nid in super_networks)\n",
    "start_time = time.time()\n",
    "with multiprocessing.Pool() as pool:\n",
    "    print(\"--- %s seconds: parallel overhead to load multiprocessing.Pool() ---\" % (time.time() - start_time))    \n",
    "    start_time = time.time()\n",
    "    results = pool.map(super_network_trace, nids)\n",
    "    print(\"--- %s seconds: parallel compute using default terminal_code ---\" % (time.time() - start_time))\n",
    "if debuglevel <= -1: print(len(results))\n",
    "if debuglevel <= -2: print(results)\n",
    "\n",
    "nids = (nid for nid in super_networks)\n",
    "snt = partial(super_network_trace, terminal_code = terminal_code)\n",
    "with multiprocessing.Pool() as pool:\n",
    "    start_time = time.time()\n",
    "    results = pool.map(snt, nids)\n",
    "    print(\"--- %s seconds: parallel compute using partial function ---\" % (time.time() - start_time))\n",
    "if debuglevel <= -1: print(len(results))\n",
    "if debuglevel <= -2: print(results)\n",
    "\n",
    "nidsWtc = ([nid,terminal_code] for nid in super_networks)\n",
    "with multiprocessing.Pool() as pool:\n",
    "    start_time = time.time()\n",
    "    results = pool.starmap(super_network_trace, nidsWtc)\n",
    "    print(\"--- %s seconds: parallel compute with list of lists and starmap ---\" % (time.time() - start_time))\n",
    "if debuglevel <= -1: print(len(results))\n",
    "if debuglevel <= -2: print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "text",
    "id": "pvfefJVLvxwe"
   },
   "source": [
    "### Colab output\n",
    "\n",
    "For the 14351 independent networks (and 2.7M segments) of the NWM Full Resolution dataset on a Google colaboratory VM, the algorithm steps through the segments of the different independent networks in order in about 4 seconds, regardless of the parallelization method. On a modest workstation at NWC with 12 cores, the compute time was approximately 2 seconds for a serial compute and 1.1 seconds for each of the parallel methods. Notably, because we are only breaking apart the independent networks, the maximum parallel speedup is limited by the size of the largest network -- the Mississippi River. By executing with the terminal node for the Mississippi River removed from the evaluation set, the serial calculation takes only 1.4 seconds and the parallel executions drop to approximately 0.4 seconds.<br>\n",
    "```\n",
    "--- 4.638037443161011 seconds: serial compute ---  \n",
    "14351  \n",
    "--- 0.0757453441619873 seconds: parallel overhead to load multiprocessing.Pool() ---  \n",
    "--- 4.344968557357788 seconds: parallel compute using default terminal_code ---  \n",
    "14351  \n",
    "--- 4.323424577713013 seconds: parallel compute using partial function ---  \n",
    "14351  \n",
    "--- 4.167566537857056 seconds: parallel compute with list of lists and starmap ---  \n",
    "14351  \n",
    "```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Network Analysis via Parallelization Demo",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
