{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "riA1WwTTID3q"
   },
   "source": [
    "## NHD Network Analysis Demo\n",
    "___TL;DR___: *We are trying to parallelize hydraulic calculations for dynamic subsets of the U.S. river and stream network*<br><br>\n",
    "The following was developed as part of the process of preparing a method for forecasting flows on the US network of rivers and streams as represented in the National Hydrography Dataset (NHD). The NHD is a continuously evolving characterization of a fractal system so we felt that we needed to plan to have some flexibility. We hope to identify the complexity inherent in the network at different levels of resolution and we hope to be able to do so dynamically. The goal is also to be able to manage the complexity calculation for arbitrary collections of headwater points, such as might be obtained from a list of named streams or during a major flood event in a particular region.<br>\n",
    "As a point of terminology, we use the word 'routing' as shorthand to refer to the computation of the translation of a particular flow condition, high or low, to downstream (or in some cases upstream) areas of influence.\n",
    "The network complexity is related to the potential for parallelization of a serial analysis of the network. We have identified three levels of parallelization that may be implemented: \n",
    "1. System-level parallelization of independent networks -- the routing computations for the Mississippi River have little (nothing, except conceptual similarity and a shared existence on earth) to do with the computations for the Columbia river for any practical level of analysis. The system of networks across the US is what we are considering in general.\n",
    "1. Network-level parallelization of interconnected reaches -- There is a need to consider the computations for adjacent branches within a network of con-flowing streams, but with proper ordering, some of the computations may be considered in parallel. For example, the Illinois River headwaters and the Mississippi River headwaters are related within their broader Mississippi network, but the routing calculations for those headwaters are pratically agnostic to one another.\n",
    "1. Reach-level parallelization of the specific routing computation -- the numerical work of routing water downstream is a matrix computation and consists of exploring solutions to differential equations, all of which may potentially be examined in parallel, under the proper conditions and with suitable assumptions.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qv_SVt3VzC9J"
   },
   "source": [
    "### Import the git repo including test data\n",
    "This git repo is a fork/branch of the national water model public repository hosted by UCAR. The UCAR repo is the basis for the WRF-Hydro model that is presently the modeling engine of the [US National Water Model.](https://water.noaa.gov/about/nwm)<br>\n",
    "\n",
    "The network analysis code assumes that the downstream neighbor is identified in the table for each stream segment as is the case for the test datasets. \n",
    "\n",
    "We recognize that others have done similar work and may possibly have done it better. We are working on being more able to nimbly respond to suggestions and opportunities for improvement. Please let us know if you see something we could do better or of course feel free to fork and improve what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "0RBaWmyY7Z88",
    "outputId": "5e88d24d-9547-4347-eb77-509384696c2b"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "try:\n",
    "    import google.colab\n",
    "    ENV_IS_CL = True\n",
    "    !git clone --single-branch --branch network https://github.com/jameshalgren/wrf_hydro_nwm_public.git\n",
    "    sys.path.append('/content/wrf_hydro_nwm_public/trunk/NDHMS/dynamic_channel_routing/src')\n",
    "    !pip install geopandas\n",
    "except:\n",
    "    ENV_IS_CL = False\n",
    "    sys.path.append(r'../src')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mJ0UirjC0iN1"
   },
   "source": [
    "### Create some general functions\n",
    "The next three blocks define interaction with the `networkbuilder` module in the git repo, which is the tool for creating the `connnections` object to characterize the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "keRYtaWBrgnz"
   },
   "outputs": [],
   "source": [
    "import networkbuilder as networkbuilder\n",
    "import recursive_print\n",
    "import os\n",
    "import geopandas as gpd\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"NHD Network traversal\n",
    "\n",
    "A demonstration version of this code is stored in this Colaboratory notebook:\n",
    "https://colab.research.google.com/github/jameshalgren/wrf_hydro_nwm_public/blob/network/trunk/NDHMS/dynamic_channel_routing/notebooks/NHD_Network_Density_Analysis.ipynb#scrollTo=h_BEdl4LID34\n",
    "\n",
    "\"\"\"\n",
    "def do_network(\n",
    "        geofile_path = None\n",
    "        , title_string = None\n",
    "        , layer_string = None\n",
    "        , driver_string = None\n",
    "        , key_col = None\n",
    "        , downstream_col = None\n",
    "        , length_col = None\n",
    "        , terminal_code = None\n",
    "        , verbose = False\n",
    "        , debuglevel = 0\n",
    "        ):\n",
    "\n",
    "    # NOTE: these methods can lose the \"connections\" and \"rows\" arguments when\n",
    "    # implemented as class methods where those arrays are members of the class.\n",
    "    if verbose: print(title_string)\n",
    "    if debuglevel <= -1: print(f'reading -- dataset: {geofile_path}; layer: {layer_string}; fiona driver: {driver_string}')\n",
    "    geofile = gpd.read_file(geofile_path, driver=driver_string, layer=layer_string)\n",
    "    if debuglevel <= -1: print(geofile.head())\n",
    "    geofile_rows = geofile.to_numpy()\n",
    "    if debuglevel <= -2: geofile.plot()\n",
    "    # Kick off recursive call for all connections and keys\n",
    "    (connections) = networkbuilder.get_down_connections(\n",
    "                    rows = geofile_rows\n",
    "                    , key_col = key_col\n",
    "                    , downstream_col = downstream_col\n",
    "                    , length_col = length_col\n",
    "                    , verbose = verbose\n",
    "                    , debuglevel = debuglevel)\n",
    "    \n",
    "    (all_keys, ref_keys, headwater_keys\n",
    "        , terminal_keys\n",
    "        , terminal_ref_keys\n",
    "        , circular_keys) = networkbuilder.determine_keys(\n",
    "                    connections = connections\n",
    "                    , rows = geofile_rows\n",
    "                    , key_col = key_col\n",
    "                    , downstream_col = downstream_col\n",
    "                    , terminal_code = terminal_code\n",
    "                    , verbose = verbose\n",
    "                    , debuglevel = debuglevel)\n",
    "    \n",
    "    (junction_keys) = networkbuilder.get_up_connections(\n",
    "                    connections = connections\n",
    "                    , terminal_code = terminal_code\n",
    "                    , headwater_keys = headwater_keys\n",
    "                    , terminal_keys = terminal_keys\n",
    "                    , verbose = verbose\n",
    "                    , debuglevel = debuglevel)\n",
    "    return connections, all_keys, ref_keys, headwater_keys \\\n",
    "        , terminal_keys, terminal_ref_keys \\\n",
    "        , circular_keys, junction_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTYCYXeVrMCo"
   },
   "outputs": [],
   "source": [
    "def do_print():    \n",
    "    recursive_print.print_basic_network_info(\n",
    "                    connections = connections_NHD\n",
    "                    , headwater_keys = headwater_keys_NHD\n",
    "                    , junction_keys = junction_keys_NHD\n",
    "                    , terminal_keys = terminal_keys_NHD\n",
    "                    , terminal_code = terminal_code_NHD\n",
    "                    , verbose = True\n",
    "                    )\n",
    "    \n",
    "    if 1 == 0: #THE RECURSIVE PRINT IS NOT A GOOD IDEA WITH LARGE NETWORKS!!!\n",
    "        recursive_print.print_connections(\n",
    "                    headwater_keys = headwater_keys_NHD\n",
    "                    , down_connections = connections_NHD\n",
    "                    , up_connections = connections_NHD\n",
    "                    , terminal_code = terminal_code_NHD\n",
    "                    , terminal_keys = terminal_keys_NHD\n",
    "                    , terminal_ref_keys = terminal_ref_keys_NHD\n",
    "                    , debuglevel = -2\n",
    "                    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "diFflaIMsef-"
   },
   "source": [
    "### Build a test case\n",
    "The `test_rows` object simulates a river network dataset such as we recieve from the National Hydrography Dataset. Each data row has a node ID, a 'to' node ID, and some other relevant data. For this test dataset, the second data column is a dummy length (and the last column could be some other value, but we haven't tried anything yet... stay tuned) and in our traversals, we can add up the lengths as a surrogate for more complex water routing functions we need to eventually manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "KFnY-vZDYPue",
    "outputId": "58c2d71e-59d2-4d4e-818b-2dea1a337471"
   },
   "outputs": [],
   "source": [
    "# def main():\n",
    "if 1 == 1:\n",
    "    \"\"\"##TEST\"\"\"\n",
    "    print(\"\")\n",
    "    print ('Executing Test')\n",
    "    # Test data\n",
    "    test_rows = [\n",
    "        [50,178,51,0],\n",
    "        [51,178,50,0],\n",
    "        [60,178,61,0],\n",
    "        [61,178,62,0],\n",
    "        [62,178,60,0],\n",
    "        [70,178,71,0],\n",
    "        [71,178,72,0],\n",
    "        [72,178,73,0],\n",
    "        [73,178,70,0],\n",
    "        [80,178,81,0],\n",
    "        [81,178,82,0],\n",
    "        [82,178,83,0],\n",
    "        [83,178,84,0],\n",
    "        [84,178,80,0],\n",
    "        [0,456,-999,0],\n",
    "        [1,178,4,0],\n",
    "        [2,394,0,0],\n",
    "        [3,301,2,0],\n",
    "        [4,798,0,0],\n",
    "        [5,679,4,0],\n",
    "        [6,523,0,0],\n",
    "        [7,815,2,0],\n",
    "        [8,841,-999,0],\n",
    "        [9,514,8,0],\n",
    "        [10,458,9,0],\n",
    "        [11,832,10,0],\n",
    "        [12,543,11,0],\n",
    "        [13,240,12,0],\n",
    "        [14,548,13,0],\n",
    "        [15,920,14,0],\n",
    "        [16,920,15,0],\n",
    "        [17,514,16,0],\n",
    "        [18,458,17,0],\n",
    "        [19,832,18,0],\n",
    "        [20,543,19,0],\n",
    "        [21,240,16,0],\n",
    "        [22,548,21,0],\n",
    "        [23,920,22,0],\n",
    "        [24,240,23,0],\n",
    "        [25,548,12,0],\n",
    "        [26,920,25,0],\n",
    "        [27,920,26,0],\n",
    "        [28,920,27,0],\n",
    "    ]\n",
    "\n",
    "    test_key_col = 0\n",
    "    test_downstream_col = 2\n",
    "    test_length_col = 1\n",
    "    test_terminal_code = -999\n",
    "    debuglevel = 0\n",
    "    verbose = True\n",
    "\n",
    "    (test_connections) = networkbuilder.get_down_connections(\n",
    "                rows = test_rows\n",
    "                , key_col = test_key_col\n",
    "                , downstream_col = test_downstream_col\n",
    "                , length_col = test_length_col\n",
    "                , verbose = True\n",
    "                , debuglevel = debuglevel\n",
    "                )\n",
    "\n",
    "    (test_all_keys, test_ref_keys, test_headwater_keys\n",
    "     , test_terminal_keys\n",
    "     , test_terminal_ref_keys\n",
    "     , test_circular_keys) = networkbuilder.determine_keys(\n",
    "                connections = test_connections\n",
    "                , rows = test_rows\n",
    "                , key_col = test_key_col\n",
    "                , downstream_col = test_downstream_col\n",
    "                , terminal_code = test_terminal_code\n",
    "                , verbose = True\n",
    "                , debuglevel = debuglevel\n",
    "                )\n",
    "\n",
    "    test_junction_keys = networkbuilder.get_up_connections(\n",
    "                connections = test_connections\n",
    "                , terminal_code = test_terminal_code\n",
    "                , headwater_keys = test_headwater_keys\n",
    "                , terminal_keys = test_terminal_keys\n",
    "                , verbose = True\n",
    "                , debuglevel = debuglevel\n",
    "                )\n",
    "\n",
    "    recursive_print.print_connections(\n",
    "                headwater_keys = test_headwater_keys\n",
    "                , down_connections = test_connections\n",
    "                , up_connections = test_connections\n",
    "                , terminal_code = test_terminal_code\n",
    "                , terminal_keys = test_terminal_keys\n",
    "                , terminal_ref_keys = test_terminal_ref_keys\n",
    "                , debuglevel = debuglevel\n",
    "                )\n",
    "\n",
    "    recursive_print.print_basic_network_info(\n",
    "                connections = test_connections\n",
    "                , headwater_keys = test_headwater_keys\n",
    "                , junction_keys = test_junction_keys\n",
    "                , terminal_keys = test_terminal_keys\n",
    "                , terminal_code = test_terminal_code\n",
    "                , verbose = True\n",
    "                , debuglevel = debuglevel\n",
    "                )\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5xhb-Q4pCbX"
   },
   "source": [
    "### Real Networks\n",
    "(you can skip this cell to test the code on the simple case generated above...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "il50SP8eZBk2",
    "outputId": "32c43141-2c53-4919-b1e6-a81dd9f6dc92"
   },
   "outputs": [],
   "source": [
    "if ENV_IS_CL: root = '/content/wrf_hydro_nwm_public/trunk/NDHMS/dynamic_channel_routing/'\n",
    "elif not ENV_IS_CL: root = os.path.dirname(os.path.abspath(''))\n",
    "test_folder = os.path.join(root, r'test')\n",
    "geo_input_folder = os.path.join(test_folder, r'input', r'geo', r'Channels')\n",
    "\n",
    "\"\"\"##NHD Subset (Brazos/Lower Colorado)\"\"\"\n",
    "Brazos_LowerColorado_ge5 = True\n",
    "\"\"\"##NHD CONUS order 5 and greater\"\"\"\n",
    "CONUS_ge5 = True\n",
    "\"\"\"These are large -- be careful\"\"\"\n",
    "CONUS_Named_Streams = False \n",
    "CONUS_FULL_RES = False\n",
    "CONUS_Named_combined = False #process the Named streams through the Full-Res paths to join the many hanging reaches\n",
    "\n",
    "debuglevel = -1\n",
    "verbose = True\n",
    "\n",
    "# The following datasets are extracts from the feature datasets available\n",
    "# from https://www.nohrsc.noaa.gov/pub/staff/keicher/NWM_live/web/data_tools/\n",
    "# the CONUS_ge5 and Brazos_LowerColorado_ge5 datasets are included\n",
    "# in the github test folder. The CONUS_Named_Streams and CONUS_FULL_RES are versions \n",
    "# of the data in the nohrsc-hosted archive but are too large to efficiently \n",
    "# package inside of the repository. \n",
    "\n",
    "if Brazos_LowerColorado_ge5:\n",
    "    nhd_conus_file_path = os.path.join(geo_input_folder\n",
    "            , r'NHD_BrazosLowerColorado_Channels.shp')\n",
    "    key_col_NHD = 2\n",
    "    downstream_col_NHD = 7\n",
    "    length_col_NHD = 6\n",
    "    terminal_code_NHD = 0\n",
    "    title_string = 'Brazos + Lower Colorado\\nNHD stream orders 5 and greater\\n'\n",
    "    title_string = 'CONUS Order 5 and Greater '\n",
    "    driver_string = 'ESRI Shapefile'\n",
    "    layer_string = 0\n",
    "\n",
    "    Brazos_LowerColorado_ge5_values = do_network (nhd_conus_file_path\n",
    "                , title_string = title_string\n",
    "                , layer_string = layer_string\n",
    "                , driver_string = driver_string\n",
    "                , key_col = key_col_NHD\n",
    "                , downstream_col = downstream_col_NHD\n",
    "                , length_col = length_col_NHD\n",
    "                , terminal_code = terminal_code_NHD\n",
    "                , verbose = verbose\n",
    "                , debuglevel = debuglevel)\n",
    "\n",
    "if CONUS_ge5:\n",
    "    nhd_conus_file_path = os.path.join(geo_input_folder\n",
    "            , r'NHD_Conus_Channels.shp')\n",
    "    key_col_NHD = 1\n",
    "    downstream_col_NHD = 6\n",
    "    length_col_NHD = 5\n",
    "    terminal_code_NHD = 0\n",
    "    title_string = 'CONUS Order 5 and Greater '\n",
    "    driver_string = 'ESRI Shapefile'\n",
    "    layer_string = 0\n",
    "\n",
    "    CONUS_ge5_values = do_network (nhd_conus_file_path\n",
    "                , title_string = title_string\n",
    "                , layer_string = layer_string\n",
    "                , driver_string = driver_string\n",
    "                , key_col = key_col_NHD\n",
    "                , downstream_col = downstream_col_NHD\n",
    "                , length_col = length_col_NHD\n",
    "                , terminal_code = terminal_code_NHD\n",
    "                , verbose = verbose\n",
    "                , debuglevel = debuglevel)\n",
    "\n",
    "if CONUS_Named_Streams:\n",
    "    nhd_conus_file_path = os.path.join(geo_input_folder\n",
    "            , r'channels_nwm_v12_routeLink_NamedOnly.shp')\n",
    "    key_col_Named_Streams = 0\n",
    "    downstream_col_Named_Streams = 5\n",
    "    length_col_Named_Streams = 4\n",
    "    terminal_code_Named_Streams = 0\n",
    "    title_string = 'NHD v1.2 segments corresponding to NHD 2.0 GNIS labeled streams\\n'\n",
    "    # driver_string = 'FileGDB'\n",
    "    driver_string = 'ESRI Shapefile'\n",
    "    # layer_string = 'named_streams_v12'\n",
    "    layer_string = 0\n",
    "\n",
    "    CONUS_Named_Streams_values = do_network (nhd_conus_file_path\n",
    "                , title_string = title_string\n",
    "                , layer_string = layer_string\n",
    "                , driver_string = driver_string\n",
    "                , key_col = key_col_Named_Streams\n",
    "                , downstream_col = downstream_col_Named_Streams\n",
    "                , length_col = length_col_Named_Streams\n",
    "                , terminal_code = terminal_code_Named_Streams\n",
    "                , verbose = verbose\n",
    "                , debuglevel = debuglevel)\n",
    "\n",
    "if CONUS_FULL_RES:\n",
    "    # nhd_conus_file_path = '../../../../../../GISTemp/nwm_v12.gdb'\n",
    "    nhd_conus_file_path = os.path.join(geo_input_folder\n",
    "            , r'channels_nwm_v12_routeLink_all.shp')\n",
    "    key_col_NHD = 0\n",
    "    downstream_col_NHD = 5\n",
    "    length_col_NHD = 4\n",
    "    terminal_code_NHD = 0\n",
    "    title_string = 'CONUS Full Resolution NWM v1.2'\n",
    "    # driver_string = 'FileGDB'\n",
    "    driver_string = 'ESRI Shapefile'\n",
    "    # layer_string = 'channels_nwm_v12_routeLink'\n",
    "    layer_string = 0\n",
    "\n",
    "    CONUS_FULL_RES_values = do_network (nhd_conus_file_path\n",
    "                , title_string = title_string\n",
    "                , layer_string = layer_string\n",
    "                , driver_string = driver_string\n",
    "                , key_col = key_col_NHD\n",
    "                , downstream_col = downstream_col_NHD\n",
    "                , length_col = length_col_NHD\n",
    "                , terminal_code = terminal_code_NHD\n",
    "                , verbose = verbose\n",
    "                , debuglevel = debuglevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "il50SP8eZBk2",
    "outputId": "32c43141-2c53-4919-b1e6-a81dd9f6dc92"
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(networkbuilder)\n",
    "import pickle\n",
    "\n",
    "CONUS_Named_combined = True\n",
    "\n",
    "if CONUS_Named_combined:\n",
    "    ''' NOW Combine the two CONUS analyses by starting with the Named Headwaters\n",
    "        but trace the network down the Full Resolution NHD. It should only work\n",
    "        if the other two datasets have been computed.'''\n",
    "    if not (CONUS_Named_Streams and CONUS_FULL_RES):\n",
    "        print('WARNING: If this works, you are using old data...')\n",
    "    \n",
    "    # Use only headwater keys that are in the full dataset.\n",
    "    headwater_keys_combined = CONUS_FULL_RES_values[3] & \\\n",
    "                                CONUS_Named_Streams_values[3]\n",
    "    # Need to make sure that these objects are independent -- we will modify them a bit.\n",
    "    connections_combined = pickle.loads(pickle.dumps(CONUS_FULL_RES_values[0]))\n",
    "    terminal_keys_combined = pickle.loads(pickle.dumps(CONUS_FULL_RES_values[4]))\n",
    "    terminal_code_combined = terminal_code_NHD\n",
    "    \n",
    "    for key in connections_combined: #Clear the upstreams and rebuild it with just named streams\n",
    "        connections_combined[key].pop('upstreams',None)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "il50SP8eZBk2",
    "outputId": "32c43141-2c53-4919-b1e6-a81dd9f6dc92"
   },
   "outputs": [],
   "source": [
    "\n",
    "    (junction_keys_combined\n",
    "     , visited_keys_combined\n",
    "     , visited_terminal_keys_combined\n",
    "     , junction_count_combined) = networkbuilder.get_up_connections(\n",
    "                 connections = connections_combined\n",
    "                 , terminal_code = terminal_code_combined\n",
    "                 , headwater_keys = headwater_keys_combined\n",
    "                 , terminal_keys = terminal_keys_combined\n",
    "                 , verbose = verbose\n",
    "                 , debuglevel = debuglevel)\n",
    "    \n",
    "#     print(len(junction_keys_combined)\n",
    "#      , len(visited_keys_combined)\n",
    "#      , len(visited_terminal_keys_combined)\n",
    "#      , junction_count_combined)\n",
    "    \n",
    "#     print(len(terminal_keys_combined - visited_terminal_keys_combined))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_fuRiEXFsvOK"
   },
   "source": [
    "### With this, we can separate the different rivers in the network\n",
    "Once a 'connection' object has been created with a representation of the river network, we can traverse that object and perform calculations -- in the example below, we parallelize the process of traversing the independent portions of the network and then serially compute the number of junctions. This corresponds to the \"**System-level parallelization**\" mentioned as _item 1_ above.\n",
    "### NOW for the next step\n",
    "We could compute total upstream length or (and this is the real goal) flow due to incoming lateral contributions from the land accumulated over the entire upstream network. That second calculation can also be parallelized but we have to figure out how to accomplish intelligently so that the collective calculation is network-aware. Such a parallelization would be the \"**Network-level parallelization of interconnected reaches**\" mentioned as _item 2_ above. The upstream length will depend on the number of upstream branches and their configuration, so there has to be some concept of stream order and topology built into the parallelization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#parallel compute\n",
    "import time\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "'''Test'''\n",
    "# terminal_code = test_terminal_code\n",
    "# terminal_keys = test_terminal_keys \n",
    "# circular_keys = test_circular_keys\n",
    "# terminal_keys_super = terminal_keys - circular_keys\n",
    "# con = test_connections\n",
    "'''Streams of NHD order 5 or greater confluent to the Brazos and Lower Colorado'''\n",
    "# terminal_code = terminal_code_NHD\n",
    "# terminal_keys = Brazos_LowerColorado_ge5_values[4] \n",
    "# circular_keys = Brazos_LowerColorado_ge5_values[6]\n",
    "# terminal_keys_super = terminal_keys - circular_keys\n",
    "# con = Brazos_LowerColorado_ge5_values[0]\n",
    "'''CONUS streams of NHD order 5 or greater'''\n",
    "terminal_code = terminal_code_NHD\n",
    "terminal_keys = CONUS_ge5_values[4] \n",
    "circular_keys = CONUS_ge5_values[6]\n",
    "terminal_keys_super = terminal_keys - circular_keys\n",
    "con = CONUS_ge5_values[0]\n",
    "'''Named Streams'''\n",
    "# terminal_keys = CONUS_Named_Streams_values[4] \n",
    "# circular_keys = CONUS_Named_Streams_values[6]\n",
    "# terminal_keys_super = terminal_keys - circular_keys\n",
    "# con = CONUS_Named_Streams_values[0]\n",
    "# terminal_code = terminal_code_NHD\n",
    "'''Full Res NHD'''\n",
    "# terminal_keys = CONUS_FULL_RES_values[4] \n",
    "# circular_keys = CONUS_FULL_RES_values[6]\n",
    "# terminal_keys_super = terminal_keys - circular_keys\n",
    "# con = CONUS_FULL_RES_values[0]\n",
    "# terminal_code = terminal_code_NHD\n",
    "'''Combined'''\n",
    "# # # NOT NEEDED # terminal_keys = terminal_keys_combined \n",
    "# # # NOT NEEDED # circular_keys = circular_keys_combined\n",
    "# terminal_keys_super = visited_terminal_keys_combined\n",
    "# con = connections_combined\n",
    "# terminal_code = terminal_code_NHD\n",
    "\n",
    "def recursive_junction_read (\n",
    "                             keys\n",
    "                             , network\n",
    "                             , terminal_code = 0\n",
    "                             , verbose = False\n",
    "                             , debuglevel = 0\n",
    "                            ):\n",
    "    global con\n",
    "    for key in keys:\n",
    "        ckey = key\n",
    "        try:\n",
    "            ukeys = con[key]['upstreams']\n",
    "            while not len(ukeys) >= 2 and not (ukeys == {terminal_code}):\n",
    "                if debuglevel <= -3: print(f\"segs at ckey {ckey}: {network['segment_count']}\")\n",
    "                # the terminal code will indicate a headwater\n",
    "                if debuglevel <= -3: print(ukeys)\n",
    "                (ckey,) = ukeys\n",
    "                ukeys = con[ckey]['upstreams']\n",
    "            if ukeys == {terminal_code}:\n",
    "                if debuglevel <= -2: print(f\"headwater found at {ckey}\")\n",
    "                network['segment_count'] += 1\n",
    "                if debuglevel <= -3: print(f\"segs at ckey {ckey}: {network['segment_count']}\")\n",
    "            elif len(ukeys) >= 2:\n",
    "                network['segment_count'] += 1\n",
    "                if debuglevel <= -2: print(f\"junction found at {ckey} with upstreams {ukeys}\")\n",
    "                network['segment_count'] += 1\n",
    "                if debuglevel <= -3: print(f\"segs at ckey {ckey}: {network['segment_count']}\")\n",
    "                network['junction_count'] += 1 #the Terminal Segment\n",
    "                recursive_junction_read (ukeys, network, terminal_code = terminal_code, verbose = verbose, debuglevel = debuglevel) \n",
    "                # print(ukeys)\n",
    "                ukeys = con[ckey]['upstreams']\n",
    "                ckey = ukeys\n",
    "        except:\n",
    "            if debuglevel <= -2: \n",
    "                print(f'There is a problem with connection: {key}: {con[key]}')\n",
    "\n",
    "def super_network_trace(\n",
    "                        nid\n",
    "                        , terminal_code = terminal_code\n",
    "                        , verbose= False\n",
    "                        , debuglevel = 0\n",
    "                        ):\n",
    "\n",
    "    network = {}\n",
    "    global con\n",
    "    us_length_total = 0\n",
    "    \n",
    "    if verbose: print(f'\\ntraversing upstream on network {nid}:')\n",
    "    # try:\n",
    "    if 1 == 1:\n",
    "        network.update({'junction_count': 0})\n",
    "        network.update({'segment_count': 0}) #the Terminal Segment\n",
    "        recursive_junction_read([nid], network, verbose = verbose, terminal_code = terminal_code, debuglevel = debuglevel)\n",
    "        if verbose: print(f\"junctions: {network['junction_count']}\")\n",
    "        if verbose: print(f\"segments: {network['segment_count']}\")\n",
    "    # except Exception as exc:\n",
    "    #     print(exc)\n",
    "    return {nid: network, 'upstream_length': us_length_total}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "Gz15rLB9WnBl",
    "outputId": "b872a326-258a-46f8-e9d6-66e0772a7957"
   },
   "outputs": [],
   "source": [
    "###continuing from previous cell\n",
    "super_networks = {terminal_key:{}\n",
    "                        for terminal_key in terminal_keys_super}\n",
    "debuglevel = -1\n",
    "verbose = False\n",
    "\n",
    "start_time = time.time()\n",
    "results_serial = {}\n",
    "for nid, network in super_networks.items():\n",
    "    network.update(super_network_trace(nid, terminal_code = terminal_code, verbose = verbose, debuglevel = debuglevel)[nid])\n",
    "print(\"--- %s seconds: serial compute ---\" % (time.time() - start_time))\n",
    "if debuglevel <= -1: print(len(super_networks.items()))\n",
    "if debuglevel <= -2: print(super_networks)\n",
    "\n",
    "    ## Notice that I'm not timing the initialization in each case, \n",
    "    ## which might be considered cheating a little bit. \n",
    "    ## I timed it for the first case for reference.\n",
    "nids = (nid for nid in super_networks)\n",
    "start_time = time.time()\n",
    "with multiprocessing.Pool() as pool:\n",
    "    print(\"--- %s seconds: parallel overhead to load multiprocessing.Pool() ---\" % (time.time() - start_time))    \n",
    "    start_time = time.time()\n",
    "    results = pool.map(super_network_trace, nids)\n",
    "    print(\"--- %s seconds: parallel compute using default terminal_code ---\" % (time.time() - start_time))\n",
    "if debuglevel <= -1: print(len(results))\n",
    "if debuglevel <= -2: print(results)\n",
    "\n",
    "nids = (nid for nid in super_networks)\n",
    "snt = partial(super_network_trace, terminal_code = terminal_code)\n",
    "with multiprocessing.Pool() as pool:\n",
    "    start_time = time.time()\n",
    "    results = pool.map(snt, nids)\n",
    "    print(\"--- %s seconds: parallel compute using partial function ---\" % (time.time() - start_time))\n",
    "if debuglevel <= -1: print(len(results))\n",
    "if debuglevel <= -2: print(results)\n",
    "\n",
    "nidsWtc = ([nid,terminal_code] for nid in super_networks)\n",
    "with multiprocessing.Pool() as pool:\n",
    "    start_time = time.time()\n",
    "    results = pool.starmap(super_network_trace, nidsWtc)\n",
    "    print(\"--- %s seconds: parallel compute with list of lists and starmap ---\" % (time.time() - start_time))\n",
    "if debuglevel <= -1: print(len(results))\n",
    "if debuglevel <= -2: print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pvfefJVLvxwe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Network Analysis via Parallelization Demo",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
